{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOTauTstPKeg",
        "outputId": "0cca45c3-62dd-4082-ab76-761d04a25f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.83\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting biopandas\n",
            "  Downloading biopandas-0.4.1-py2.py3-none-any.whl (878 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m879.0/879.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from biopandas) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from biopandas) (1.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from biopandas) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->biopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->biopandas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->biopandas) (1.16.0)\n",
            "Installing collected packages: biopandas\n",
            "Successfully installed biopandas-0.4.1\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.3-cp310-cp310-manylinux1_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.3\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython\n",
        "!pip install  networkx\n",
        "!pip install biopandas\n",
        "!pip install dgl\n",
        "!pip install torch_geometric\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from Bio import PDB\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "from biopandas.pdb import PandasPdb\n",
        "from torch_geometric.data import Data,DataLoader\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import dgl\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the dataset saved in numpy array conatins\n",
        "#Protein1 id  ; protein 2 id ; 0 or 1\n",
        "\n",
        "dataset = np.load('dataset.npy', allow_pickle=True)\n",
        "protein_1 = dataset[:,2]\n",
        "protein_2 = dataset[:,5]\n",
        "label = dataset[:,6].astype(float)"
      ],
      "metadata": {
        "id": "fApt1_NebIf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3fdKz3fit0J",
        "outputId": "ff14b3fa-27b4-4a9b-bce0-2330e0424f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. ... 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZvaVYg7Q17s"
      },
      "outputs": [],
      "source": [
        "#Download the pdf files from the rcsb api\n",
        "# if you want to download all the pdbs use\n",
        "# list(pdbs=set(protein_1).union(set(protein_2)))\n",
        "\n",
        "\n",
        "def download_pdb_files(pdb_ids, destination_folder):\n",
        "    base_url = \"https://files.rcsb.org/download/\"\n",
        "\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "    index=0\n",
        "    for pdb_id in pdb_ids:\n",
        "        print(index)\n",
        "        index+=1\n",
        "        url = f\"{base_url}{pdb_id.upper()}.pdb\"\n",
        "        file_path = os.path.join(destination_folder, f\"{pdb_id.upper()}.pdb\")\n",
        "\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded: {pdb_id}\")\n",
        "        else:\n",
        "            print(f\"Failed to download: {pdb_id} (Status Code: {response.status_code})\")\n",
        "\n",
        "# Example: Download PDB files for proteins with IDs 1abc and 1xyz\n",
        "pdb_ids_to_download = pdbs\n",
        "download_destination = 'pdbs'\n",
        "download_pdb_files(pdb_ids_to_download, download_destination)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJlmcR5DWEnj"
      },
      "outputs": [],
      "source": [
        "def load_pickle(path):\n",
        "  v=None\n",
        "  with open(path, 'rb') as file:\n",
        "    v=pickle.load(file)\n",
        "    return v\n",
        "def save_pickle(path,var):\n",
        "  with open(path, 'wb') as file:\n",
        "    pickle.dump(var, file)\n",
        "\n",
        "\n",
        "#to execute when finishing and saving evrything to drive\n",
        "#to save the adjancy and embeddings dict\n",
        "# save_pickle('adj_dict.pkl',adj_dict)\n",
        "# save_pickle('embed_dict.pkl',embed_dict)\n",
        "#copy the preprocessed pdbs from drive\n",
        "# !cp *.pkl ./drive/MyDrive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTC-Y1hBsPok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTMOhHQy98m4"
      },
      "outputs": [],
      "source": [
        "# import pdbs from drive or you  can download them using download_pdb_files function\n",
        "# !cp -r ./drive/MyDrive/pdbs .\n",
        "# !cp ./drive/MyDrive/*.pkl  .\n",
        "#read the files\n",
        "adj_dict=load_pickle('adj_dict.pkl')\n",
        "embed_dict=load_pickle('embed_dict.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the pdbs folder from drive\n",
        "!cp -r ./drive/MyDrive/pdbs ."
      ],
      "metadata": {
        "id": "R1Tpg9PHd_gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tecj6sYTquoZ"
      },
      "outputs": [],
      "source": [
        "#Amino acids embeddings from the paper researchers Github\n",
        "pcp_dict = {'ALA':[ 0.62014, -0.18875, -1.2387, -0.083627, -1.3296, -1.3817, -0.44118],\n",
        "            'CYS':[0.29007, -0.44041,-0.76847, -1.05, -0.4893, -0.77494, -1.1148],\n",
        "            'ASP':[-0.9002, 1.5729, -0.89497, 1.7376, -0.72498, -0.50189, -0.91814],\n",
        "            'GLU':[-0.74017, 1.5729, -0.28998, 1.4774, -0.25361, 0.094051, -0.4471],\n",
        "            'PHE':[1.1903, -1.1954, 1.1812, -1.1615, 1.1707, 0.8872, 0.02584],\n",
        "            'GLY':[ 0.48011, 0.062916, -1.9949, 0.25088, -1.8009, -2.0318, 2.2022],\n",
        "            'HIS':[-0.40009, -0.18875, 0.17751, 0.77123, 0.5559, 0.44728, -0.71617],\n",
        "            'ILE':[1.3803, -0.84308, 0.57625, -1.1615, 0.10503, -0.018637, -0.21903],\n",
        "            'LYS':[-1.5003, 1.5729, 0.75499, 1.1057, 0.44318, 0.95221, -0.27937],\n",
        "            'LEU':[1.0602, -0.84308, 0.57625, -1.273, 0.10503, 0.24358, 0.24301],\n",
        "            'MET':[0.64014, -0.59141, 0.59275, -0.97565, 0.46368, 0.46679, -0.51046],\n",
        "            'ASN':[-0.78018, 1.0696, -0.38073, 1.2172, -0.42781, -0.35453, -0.46879],\n",
        "            'PRO':[0.12003, 0.062916, -0.84272, -0.1208, -0.45855, -0.75977, 3.1323],\n",
        "            'GLN':[-0.85019, 0.16358, 0.22426, 0.8084, 0.04355, 0.24575, 0.20516],\n",
        "            'ARG':[-2.5306, 1.5729, 0.89249, 0.8084, 1.181, 1.6067, 0.11866],\n",
        "            'SER':[-0.18004, 0.21392, -1.1892, 0.32522, -1.1656, -1.1282, -0.48056],\n",
        "            'THR':[-0.050011, -0.13842, -0.58422, 0.10221, -0.69424, -0.63625, -0.50017],\n",
        "            'VAL':[1.0802, -0.69208, -0.028737, -0.90132, -0.36633, -0.3762, 0.32502],\n",
        "            'TRP':[0.81018, -1.6484, 2.0062, -1.0872, 2.3901, 1.8299, 0.032377],\n",
        "            'TYR':[0.26006, -1.0947, 1.2307, -0.78981, 1.2527, 1.1906, -0.18876]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QNUvSdrGijo"
      },
      "outputs": [],
      "source": [
        "#The function that parses the pdb files to adjancy matrix of amino acids\n",
        "\n",
        "aa_set = set([\n",
        "        'ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE',\n",
        "        'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL',\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "def pdb_to_adjacency_matrix(pdb_file):\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('protein', pdb_file)\n",
        "\n",
        "    amino_acids = {}\n",
        "    amino_acid_index = 0\n",
        "\n",
        "    atom_coordinates = []\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if PDB.is_aa(residue):\n",
        "                    amino_acid = f\"{residue.get_resname()}_{chain.id}_{residue.id[1]}\"\n",
        "                    if amino_acid not in amino_acids:\n",
        "                        amino_acids[amino_acid] = amino_acid_index\n",
        "                        amino_acid_index += 1\n",
        "\n",
        "                    for atom in residue:\n",
        "                        if atom.name in {'N', 'CA', 'C', 'O'}:\n",
        "                            atom_coordinates.append(atom.coord)\n",
        "\n",
        "    num_amino_acids = len(amino_acids)\n",
        "    adjacency_matrix = np.zeros((num_amino_acids, num_amino_acids), dtype=int)\n",
        "\n",
        "    for i in range(num_amino_acids):\n",
        "        for j in range(i + 1, num_amino_acids):\n",
        "            distance = np.linalg.norm(atom_coordinates[i] - atom_coordinates[j])\n",
        "            if distance < 10.0:\n",
        "                adjacency_matrix[i, j] = 1\n",
        "                adjacency_matrix[j, i] = 1\n",
        "    embed=[]\n",
        "    for i in amino_acids.keys():\n",
        "        if i[:3] in aa_set:\n",
        "            embed.append(pcp_dict[i[:3]])\n",
        "        else:embed.append([0]*7)\n",
        "\n",
        "    return adjacency_matrix, embed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyMd4eXqUT3W"
      },
      "outputs": [],
      "source": [
        "#create dataset from availiable pdbs (filter failed to download)\n",
        "# use this to create dataset from tha all the pdbs availiable in pdbs folder availiable=[i[:4]for i in os.listdir('pdbs')]\n",
        "\n",
        "import os\n",
        "#create dataset from only preprocessed pdbs\n",
        "availiable=[i[:4]for i in adj_dict]\n",
        "n=len(protein_1)\n",
        "p1=[]\n",
        "p2=[]\n",
        "labels=[]\n",
        "for i in range(n):\n",
        "    if protein_1[i] in availiable and protein_2[i] in availiable:\n",
        "        p1.append(protein_1[i])\n",
        "        p2.append(protein_2[i])\n",
        "        labels.append(label[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg5Rn5Ygz0N6",
        "outputId": "6cb9620d-12cd-421f-9557-8c97952d9608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3711"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQUSPI1ZXUbj"
      },
      "outputs": [],
      "source": [
        "#skip this if you choose to use the  reprocessed\n",
        "needed=set(p1).union(set(p2))\n",
        "# adj_dict={}\n",
        "# embed_dict={}\n",
        "j=0\n",
        "for i in list(needed)[1012:]:\n",
        "    if i not in adj_dict:\n",
        "      a,b=pdb_to_adjacency_matrix('pdbs/'+i+'.pdb')\n",
        "      adj_dict[i]=torch.tensor(a).int().nonzero().t()\n",
        "      embed_dict[i]=torch.tensor(b)\n",
        "      print(j)\n",
        "      j+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to ensure that all the tensors are in nonzero format\n",
        "for i in adj_dict:\n",
        "  if adj_dict[i].shape[0]>2:\n",
        "    adj_dict[i]=torch.tensor(adj_dict[i]).int().nonzero().t()"
      ],
      "metadata": {
        "id": "Pa4h7OOM0fx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXtUcdD16WGx"
      },
      "outputs": [],
      "source": [
        "\n",
        "protein_1=[]\n",
        "protein_2=[]\n",
        "protein_embed_1=[]\n",
        "protein_embed_2=[]\n",
        "for i,j in zip(p1,p2):\n",
        "    protein_1.append(adj_dict[i])\n",
        "    protein_embed_1.append(embed_dict[i])\n",
        "    protein_2.append(adj_dict[j])\n",
        "    protein_embed_2.append(embed_dict[j])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "protein_embed_1 = [torch.tensor(i).float() for i in protein_embed_1]\n",
        "protein_embed_2 = [torch.tensor(i).float() for i in protein_embed_2]"
      ],
      "metadata": {
        "id": "7JYO7u8pPLQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42d78f7-566d-4158-9169-d5d687de36fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1cc5120ea638>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  protein_embed_1 = [torch.tensor(i).float() for i in protein_embed_1]\n",
            "<ipython-input-16-1cc5120ea638>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  protein_embed_2 = [torch.tensor(i).float() for i in protein_embed_2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_1 = [torch.zeros(i.shape[0]).to(torch.int64) for i in protein_embed_1]\n",
        "batch_2 = [torch.ones(i.shape[0]).to(torch.int64) for i in protein_embed_2]"
      ],
      "metadata": {
        "id": "Hi9eTu2vl0AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=torch.tensor(labels)"
      ],
      "metadata": {
        "id": "F7zggK0lIB8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "3YpdHjg4RVfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "\n",
        "class ProteinComparer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ProteinComparer, self).__init__()\n",
        "        self.conv = GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
        "        self.fc = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, adj1, adj2, embedding1, embedding2):\n",
        "        # Graph 1\n",
        "        x1 = self.conv(embedding1, adj1)\n",
        "        x1 = global_add_pool(x1, torch.zeros_like(embedding1[:, 0], dtype=torch.long))\n",
        "\n",
        "        # Graph 2\n",
        "        x2 = self.conv(embedding2, adj2)\n",
        "        x2 = global_add_pool(x2, torch.zeros_like(embedding2[:, 0], dtype=torch.long))\n",
        "\n",
        "        # Concatenate or perform any operation you need to compare the two graphs\n",
        "        x_combined = torch.cat([x1, x2], dim=1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        output = self.fc(x_combined)\n",
        "\n",
        "        # Apply sigmoid activation using F.sigmoid\n",
        "        output = output\n",
        "\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 7  # Dimension of node embeddings\n",
        "hidden_dim = 64  # Hidden dimension in GNN layers\n",
        "output_dim = 1  # Output dimension, 1 for binary classification\n",
        "model = ProteinComparer(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n"
      ],
      "metadata": {
        "id": "mM2aSC1XFhMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vzqhMdQcAX2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H-c9jhWCfBn"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "hist=[]\n",
        "\n",
        "for k in range(100):\n",
        "  loss1=0\n",
        "  ids=[random.randint(0,3200) for _ in range(64)]\n",
        "  for i in ids:\n",
        "    adj1=protein_1[i]\n",
        "    adj2=protein_2[i]\n",
        "    emb1=protein_embed_1[i]\n",
        "    emb2=protein_embed_2[i]\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(adj1, adj2, emb1, emb2)\n",
        "    loss = criterion(output, labels[i].view(-1, 1).float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss1+=loss.item()\n",
        "  j=j+1\n",
        "  hist.append(loss1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist)"
      ],
      "metadata": {
        "id": "5KJcV88xVV7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB5AlEUEEMIJ",
        "outputId": "6996ff11-e732-44be-952d-a02e45b813d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05\n"
          ]
        }
      ],
      "source": [
        "res=0\n",
        "model.eval()\n",
        "for i in range(400):\n",
        "  adj1=protein_1[i]\n",
        "  adj2=protein_2[i]\n",
        "  emb1=protein_embed_1[i]\n",
        "  emb2=protein_embed_2[i]\n",
        "  output = model(adj1, adj2, emb1, emb2).detach()\n",
        "  # print(f\"Sample {i + 1}: True Label = {labels[i].item()}, Predicted Label = {output[0][0].item()}\")\n",
        "  pred=0 if output[0][0].item()<0 else 1\n",
        "  res+= 1 if labels[i].item() == pred else 0\n",
        "\n",
        "print(res/400)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention GNN"
      ],
      "metadata": {
        "id": "FKzMKjn4Rn8P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j61k1CInI-M3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "class AttGNN(nn.Module):\n",
        "    def __init__(self, n_output=1, num_features_pro= 1024, output_dim=128, dropout=0.1, heads = 1 ):\n",
        "        super(AttGNN, self).__init__()\n",
        "\n",
        "        print('AttGNN Loaded')\n",
        "\n",
        "        self.hidden = 8\n",
        "        self.heads = 1\n",
        "\n",
        "        # for protein 1\n",
        "        self.pro1_conv1 = GATConv(num_features_pro, self.hidden* 16, heads=self.heads, dropout=0.2)\n",
        "        self.pro1_fc1 = nn.Linear(128, output_dim)\n",
        "\n",
        "\n",
        "        # for protein 2\n",
        "        self.pro2_conv1 = GATConv(num_features_pro, self.hidden*16, heads=self.heads, dropout=0.2)\n",
        "        self.pro2_fc1 = nn.Linear(128, output_dim)\n",
        "\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # combined layers\n",
        "        self.fc1 = nn.Linear(output_dim*3, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.out = nn.Linear(64, n_output)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, pro1_data, pro2_data):\n",
        "\n",
        "        # get graph input for protein 1\n",
        "        pro1_x, pro1_edge_index, pro1_batch = pro1_data.x, pro1_data.edge_index, pro1_data.batch\n",
        "        # get graph input for protein 2\n",
        "        pro2_x, pro2_edge_index, pro2_batch = pro2_data.x, pro2_data.edge_index, pro2_data.batch\n",
        "\n",
        "\n",
        "        x = self.pro1_conv1(pro1_x, pro1_edge_index)\n",
        "        x = self.relu(x)\n",
        "\n",
        "\t# global pooling\n",
        "        x = gep(x, pro1_batch)\n",
        "\n",
        "        # flatten\n",
        "        x = self.relu(self.pro1_fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "        xt = self.pro2_conv1(pro2_x, pro2_edge_index)\n",
        "        xt = self.relu(self.pro2_fc1(xt))\n",
        "\n",
        "\t# global pooling\n",
        "        xt = gep(xt, pro2_batch)\n",
        "\n",
        "        # flatten\n",
        "        xt = self.relu(xt)\n",
        "        xt = self.dropout(xt)\n",
        "\n",
        "\t# Concatenation\n",
        "        xc = torch.cat((x, xt), 0)\n",
        "        xc=xc.flatten()\n",
        "\n",
        "        # add some dense layers\n",
        "        xc = self.fc1(xc)\n",
        "        # xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        xc = self.fc2(xc)\n",
        "        # xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        out = self.out(xc)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attgnn=AttGNN(num_features_pro=7)"
      ],
      "metadata": {
        "id": "LgWlUh3u2tar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5fe6f0-d2e2-4655-fe6c-156a7c8d1cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttGNN Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RQ_S8jQkuucU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPS0HNPmN8Qy"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = optim.Adam(attgnn.parameters())\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wvF4WLI0-Tc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "attgnn.train()\n",
        "hist=[]\n",
        "\n",
        "for k in range(100):\n",
        "  loss1=0\n",
        "  ids=[random.randint(0,3200) for _ in range(64)]\n",
        "  for i in ids:\n",
        "    pr1=Data(x=protein_embed_1[i],edge_index=protein_1[i],batch=batch_1[i])\n",
        "    pr2=Data(x=protein_embed_2[i],edge_index=protein_2[i],batch=batch_2[i])\n",
        "    optimizer.zero_grad()\n",
        "    output = attgnn(pr1,pr2)\n",
        "    loss = criterion(output, torch.tensor([labels[i]]).float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss1+=loss.item()\n",
        "  print(f\"epoch {k}  sum of loss {loss1}\")\n",
        "  hist.append(loss1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist)"
      ],
      "metadata": {
        "id": "Pz_1alxV8sy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh7SyasvOYcZ"
      },
      "outputs": [],
      "source": [
        "#eval\n",
        "res=0\n",
        "attgnn.eval()\n",
        "for i in range(3200,3500):\n",
        "  pr1=Data(x=protein_embed_1[i],edge_index=protein_1[i],batch=batch_1[i])\n",
        "  pr2=Data(x=protein_embed_2[i],edge_index=protein_2[i],batch=batch_2[i])\n",
        "  output = attgnn(pr1,pr2).detach()\n",
        "  # print(f\"Sample {i + 1}: True Label = {labels[i].item()}, Predicted Label = {output[0][0].item()}\")\n",
        "  pred= output.item()>0\n",
        "  res+=  labels[i].item() == pred\n",
        "\n",
        "print(res/300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONV GNN"
      ],
      "metadata": {
        "id": "-7ZVP1hDR0i-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQkaYfxtSA0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47e26e2-a9cd-4fce-c999-88726c21c1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNN Loaded\n",
            "GCNN(\n",
            "  (pro1_conv1): GCNConv(7, 7)\n",
            "  (pro1_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (pro2_conv1): GCNConv(7, 7)\n",
            "  (pro2_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (relu): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.03, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (fc1): Linear(in_features=384, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Building model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "\n",
        "\n",
        "class GCNN(nn.Module):\n",
        "    def __init__(self, n_output=1, num_features_pro= 7, output_dim=128, dropout=0.03):\n",
        "        super(GCNN, self).__init__()\n",
        "\n",
        "        print('GCNN Loaded')\n",
        "\n",
        "        # for protein 1\n",
        "        self.n_output = n_output\n",
        "        self.pro1_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
        "        self.pro1_fc1 = nn.Linear(num_features_pro, output_dim)\n",
        "\n",
        "        # for protein 2\n",
        "        self.pro2_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
        "        self.pro2_fc1 = nn.Linear(num_features_pro, output_dim)\n",
        "\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # combined layers\n",
        "        self.fc1 = nn.Linear(3 * output_dim, 256)\n",
        "        self.fc2 = nn.Linear(256 ,64)\n",
        "        self.out = nn.Linear(64, self.n_output)\n",
        "\n",
        "    def forward(self, pro1_data, pro2_data):\n",
        "\n",
        "        #get graph input for protein 1\n",
        "        pro1_x, pro1_edge_index, pro1_batch = pro1_data.x, pro1_data.edge_index, pro1_data.batch\n",
        "        # get graph input for protein 2\n",
        "        pro2_x, pro2_edge_index, pro2_batch = pro2_data.x, pro2_data.edge_index, pro2_data.batch\n",
        "\n",
        "        x = self.pro1_conv1(pro1_x, pro1_edge_index)\n",
        "        x = self.relu(x)\n",
        "\n",
        "\t# global pooling\n",
        "        x = gep(x, pro1_batch)\n",
        "\n",
        "        # flatten\n",
        "        x = self.relu(self.pro1_fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        xt = self.pro2_conv1(pro2_x, pro2_edge_index)\n",
        "        xt = self.relu(xt)\n",
        "\n",
        "\t# global pooling\n",
        "        xt = gep(xt, pro2_batch)\n",
        "\n",
        "        # flatten\n",
        "        xt = self.relu(self.pro2_fc1(xt))\n",
        "        xt = self.dropout(xt)\n",
        "\n",
        "\t# Concatenation\n",
        "        xc = torch.cat((x, xt), 0)\n",
        "        xc=xc.flatten()\n",
        "\n",
        "        # add some dense layers\n",
        "        xc = self.fc1(xc)\n",
        "        xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        xc = self.fc2(xc)\n",
        "        xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        out = self.out(xc)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "gcnn = GCNN()\n",
        "print(gcnn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gcnn.load_state_dict(torch.load('gcnn (2).pth'))\n",
        "print(gcnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB2Gi4CQ1FZI",
        "outputId": "c7b674ad-9f73-44b4-81a3-516b2f497302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNN(\n",
            "  (pro1_conv1): GCNConv(7, 7)\n",
            "  (pro1_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (pro2_conv1): GCNConv(7, 7)\n",
            "  (pro2_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (relu): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.03, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (fc1): Linear(in_features=384, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ygIw4jT_4p"
      },
      "outputs": [],
      "source": [
        "gcnn_optimizer = optim.Adam(gcnn.parameters())\n",
        "gcnn_criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpOiernbSvfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b26dea7-8c68-4f63-afe3-f0d1a3550a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  sum of loss 18.94831371033954\n",
            "epoch 1  sum of loss 21.67714976707513\n",
            "epoch 2  sum of loss 18.695245682566167\n",
            "epoch 3  sum of loss 19.264143897219665\n",
            "epoch 4  sum of loss 21.12277850491849\n",
            "epoch 5  sum of loss 20.560761001271228\n",
            "epoch 6  sum of loss 16.22201753271199\n",
            "epoch 7  sum of loss 17.9620258244188\n",
            "epoch 8  sum of loss 13.942318693788861\n",
            "epoch 9  sum of loss 15.009433549319404\n",
            "epoch 10  sum of loss 19.384062811980943\n",
            "epoch 11  sum of loss 16.618564643804895\n",
            "epoch 12  sum of loss 10.592217939005657\n",
            "epoch 13  sum of loss 21.35349029921571\n",
            "epoch 14  sum of loss 15.996947791821379\n",
            "epoch 15  sum of loss 18.105617499983094\n",
            "epoch 16  sum of loss 17.389514881162498\n",
            "epoch 17  sum of loss 23.636711110424155\n",
            "epoch 18  sum of loss 8.831915781676864\n",
            "epoch 19  sum of loss 15.111227701866092\n",
            "epoch 20  sum of loss 20.171395283269916\n",
            "epoch 21  sum of loss 14.689115026058243\n",
            "epoch 22  sum of loss 18.432004666219278\n",
            "epoch 23  sum of loss 21.546478514339448\n",
            "epoch 24  sum of loss 12.053071369636923\n",
            "epoch 25  sum of loss 14.900067564418215\n",
            "epoch 26  sum of loss 20.955501163557514\n",
            "epoch 27  sum of loss 20.919934900523934\n",
            "epoch 28  sum of loss 18.890827858894934\n",
            "epoch 29  sum of loss 17.054716573188088\n",
            "epoch 30  sum of loss 19.439090932585977\n",
            "epoch 31  sum of loss 13.620636843274399\n",
            "epoch 32  sum of loss 22.67830675875058\n",
            "epoch 33  sum of loss 14.336801468767261\n",
            "epoch 34  sum of loss 16.257292837028395\n",
            "epoch 35  sum of loss 15.81768363242846\n",
            "epoch 36  sum of loss 12.819733895294563\n",
            "epoch 37  sum of loss 14.39874802623307\n",
            "epoch 38  sum of loss 16.53316861451556\n",
            "epoch 39  sum of loss 17.976430597450882\n",
            "epoch 40  sum of loss 20.65456734801478\n",
            "epoch 41  sum of loss 16.56185175139708\n",
            "epoch 42  sum of loss 15.273848012448374\n",
            "epoch 43  sum of loss 15.165931320310733\n",
            "epoch 44  sum of loss 15.278331540621439\n",
            "epoch 45  sum of loss 12.590837153994844\n",
            "epoch 46  sum of loss 17.330814674031252\n",
            "epoch 47  sum of loss 16.035074820292675\n",
            "epoch 48  sum of loss 13.367171559498658\n",
            "epoch 49  sum of loss 15.910770304988393\n",
            "epoch 50  sum of loss 11.85059555766523\n",
            "epoch 51  sum of loss 12.015139523433344\n",
            "epoch 52  sum of loss 14.549169684095151\n",
            "epoch 53  sum of loss 10.95538419473185\n",
            "epoch 54  sum of loss 9.650082478432847\n",
            "epoch 55  sum of loss 9.643676848550019\n",
            "epoch 56  sum of loss 14.470170411577309\n",
            "epoch 57  sum of loss 14.507978696187841\n",
            "epoch 58  sum of loss 15.575731428672675\n",
            "epoch 59  sum of loss 17.86364371189434\n",
            "epoch 60  sum of loss 13.857847324505881\n",
            "epoch 61  sum of loss 16.203428247458547\n",
            "epoch 62  sum of loss 14.09605406801398\n",
            "epoch 63  sum of loss 12.687857223744212\n",
            "epoch 64  sum of loss 15.091759239293092\n",
            "epoch 65  sum of loss 8.50359015736527\n",
            "epoch 66  sum of loss 12.63400145793073\n",
            "epoch 67  sum of loss 17.43608123064563\n",
            "epoch 68  sum of loss 15.670230098604737\n",
            "epoch 69  sum of loss 9.28054325989136\n",
            "epoch 70  sum of loss 11.22763124534027\n",
            "epoch 71  sum of loss 10.000825691013077\n",
            "epoch 72  sum of loss 8.8919523898516\n",
            "epoch 73  sum of loss 13.142184397430473\n",
            "epoch 74  sum of loss 15.226169608999072\n",
            "epoch 75  sum of loss 12.202367743693246\n",
            "epoch 76  sum of loss 13.346815240914067\n",
            "epoch 77  sum of loss 12.416648701749864\n",
            "epoch 78  sum of loss 19.49268786295376\n",
            "epoch 79  sum of loss 7.478762681271771\n",
            "epoch 80  sum of loss 13.965488147077156\n",
            "epoch 81  sum of loss 11.447544739768913\n",
            "epoch 82  sum of loss 16.87843390219175\n",
            "epoch 83  sum of loss 14.435746694055368\n",
            "epoch 84  sum of loss 11.681513190221349\n",
            "epoch 85  sum of loss 10.442005462242154\n",
            "epoch 86  sum of loss 17.018768932132122\n",
            "epoch 87  sum of loss 11.677085828488082\n",
            "epoch 88  sum of loss 17.746705142652832\n",
            "epoch 89  sum of loss 21.225473648334763\n",
            "epoch 90  sum of loss 16.696438259250947\n",
            "epoch 91  sum of loss 8.755920097433648\n",
            "epoch 92  sum of loss 13.789146977587816\n",
            "epoch 93  sum of loss 13.358693039519716\n",
            "epoch 94  sum of loss 10.147120043167124\n",
            "epoch 95  sum of loss 11.605608894658719\n",
            "epoch 96  sum of loss 17.06681592264928\n",
            "epoch 97  sum of loss 14.776089044925529\n",
            "epoch 98  sum of loss 17.55513177117289\n",
            "epoch 99  sum of loss 12.297504277470917\n",
            "epoch 100  sum of loss 15.677405304276515\n",
            "epoch 101  sum of loss 11.404655438601466\n",
            "epoch 102  sum of loss 9.917824975805072\n",
            "epoch 103  sum of loss 13.645847896512343\n",
            "epoch 104  sum of loss 14.975518074989633\n",
            "epoch 105  sum of loss 15.114868104301138\n",
            "epoch 106  sum of loss 17.712473579379523\n",
            "epoch 107  sum of loss 13.120230245256113\n",
            "epoch 108  sum of loss 6.439992710723313\n",
            "epoch 109  sum of loss 9.004254458506846\n",
            "epoch 110  sum of loss 11.538633540754116\n",
            "epoch 111  sum of loss 13.91490829126025\n",
            "epoch 112  sum of loss 5.140458307618161\n",
            "epoch 113  sum of loss 13.641455809347597\n",
            "epoch 114  sum of loss 12.119230286763589\n",
            "epoch 115  sum of loss 12.695089826443166\n",
            "epoch 116  sum of loss 9.653389096267011\n",
            "epoch 117  sum of loss 7.989354269642985\n",
            "epoch 118  sum of loss 15.33035520162729\n",
            "epoch 119  sum of loss 10.60365642178738\n",
            "epoch 120  sum of loss 12.910188857515871\n",
            "epoch 121  sum of loss 13.201987616771719\n",
            "epoch 122  sum of loss 12.766919860909063\n",
            "epoch 123  sum of loss 9.853786446324396\n",
            "epoch 124  sum of loss 8.705559966641744\n",
            "epoch 125  sum of loss 10.1514930905862\n",
            "epoch 126  sum of loss 11.318170240740054\n",
            "epoch 127  sum of loss 11.231545748012971\n",
            "epoch 128  sum of loss 16.604729451916782\n",
            "epoch 129  sum of loss 17.667503758896256\n",
            "epoch 130  sum of loss 11.1537867219701\n",
            "epoch 131  sum of loss 11.301318649631433\n",
            "epoch 132  sum of loss 16.440853849222005\n",
            "epoch 133  sum of loss 9.812248629277745\n",
            "epoch 134  sum of loss 11.866307162701318\n",
            "epoch 135  sum of loss 14.297551915004979\n",
            "epoch 136  sum of loss 7.526202366163632\n",
            "epoch 137  sum of loss 11.593013799400255\n",
            "epoch 138  sum of loss 10.205056691323602\n",
            "epoch 139  sum of loss 10.123487728985896\n",
            "epoch 140  sum of loss 13.327246986028726\n",
            "epoch 141  sum of loss 7.305845536179815\n",
            "epoch 142  sum of loss 14.308053350626336\n",
            "epoch 143  sum of loss 17.91778842232593\n",
            "epoch 144  sum of loss 13.556632155458324\n",
            "epoch 145  sum of loss 14.268321751037073\n",
            "epoch 146  sum of loss 16.962830311753695\n",
            "epoch 147  sum of loss 8.196863645194234\n",
            "epoch 148  sum of loss 15.277742137049952\n",
            "epoch 149  sum of loss 8.726180270861112\n",
            "epoch 150  sum of loss 9.985190655226003\n",
            "epoch 151  sum of loss 12.253629143211887\n",
            "epoch 152  sum of loss 11.397296539105088\n",
            "epoch 153  sum of loss 12.185997599948477\n",
            "epoch 154  sum of loss 7.385325414862494\n",
            "epoch 155  sum of loss 9.494383610111473\n",
            "epoch 156  sum of loss 7.952195205093617\n",
            "epoch 157  sum of loss 9.933451531046767\n",
            "epoch 158  sum of loss 10.529640726014378\n",
            "epoch 159  sum of loss 6.879091030272852\n",
            "epoch 160  sum of loss 10.61508200246251\n",
            "epoch 161  sum of loss 9.390009874159551\n",
            "epoch 162  sum of loss 6.500955651577794\n",
            "epoch 163  sum of loss 11.612958258628797\n",
            "epoch 164  sum of loss 7.398717817468372\n",
            "epoch 165  sum of loss 11.442735708286506\n",
            "epoch 166  sum of loss 15.108171915348613\n",
            "epoch 167  sum of loss 13.773292155048392\n",
            "epoch 168  sum of loss 15.503179806417869\n",
            "epoch 169  sum of loss 11.61583732701138\n",
            "epoch 170  sum of loss 11.553613231442338\n",
            "epoch 171  sum of loss 8.907328835119309\n",
            "epoch 172  sum of loss 17.01417330748291\n",
            "epoch 173  sum of loss 15.56882236781878\n",
            "epoch 174  sum of loss 11.348961802044702\n",
            "epoch 175  sum of loss 12.723067873378907\n",
            "epoch 176  sum of loss 8.87414691531535\n",
            "epoch 177  sum of loss 11.546053330681579\n",
            "epoch 178  sum of loss 10.695857026245584\n",
            "epoch 179  sum of loss 13.991395647340969\n",
            "epoch 180  sum of loss 7.498498553034275\n",
            "epoch 181  sum of loss 8.64066173207657\n",
            "epoch 182  sum of loss 9.754348665905713\n",
            "epoch 183  sum of loss 13.798956758641697\n",
            "epoch 184  sum of loss 14.191956258683732\n",
            "epoch 185  sum of loss 8.670561166253794\n",
            "epoch 186  sum of loss 9.559465799395198\n",
            "epoch 187  sum of loss 10.216753210728596\n",
            "epoch 188  sum of loss 9.716048596756504\n",
            "epoch 189  sum of loss 10.631210854627621\n",
            "epoch 190  sum of loss 12.506664969903628\n",
            "epoch 191  sum of loss 9.047577362927017\n",
            "epoch 192  sum of loss 9.671557223184138\n",
            "epoch 193  sum of loss 16.144285877987635\n",
            "epoch 194  sum of loss 14.865524717084536\n",
            "epoch 195  sum of loss 9.87578822296697\n",
            "epoch 196  sum of loss 10.982237059711917\n",
            "epoch 197  sum of loss 15.441884702886876\n",
            "epoch 198  sum of loss 13.006161298779038\n",
            "epoch 199  sum of loss 13.926282548871978\n",
            "epoch 200  sum of loss 7.0176656289810095\n",
            "epoch 201  sum of loss 10.231569471940542\n",
            "epoch 202  sum of loss 9.602796980772297\n",
            "epoch 203  sum of loss 9.471490527340297\n",
            "epoch 204  sum of loss 10.971539145845957\n",
            "epoch 205  sum of loss 11.90381045436359\n",
            "epoch 206  sum of loss 9.41443667398934\n",
            "epoch 207  sum of loss 10.13421996443737\n",
            "epoch 208  sum of loss 12.06725724339198\n",
            "epoch 209  sum of loss 10.577289849363991\n",
            "epoch 210  sum of loss 9.902820801009879\n",
            "epoch 211  sum of loss 10.13135988314992\n",
            "epoch 212  sum of loss 10.57087076892518\n",
            "epoch 213  sum of loss 12.609264425178193\n",
            "epoch 214  sum of loss 11.670467309500145\n",
            "epoch 215  sum of loss 11.99373486562537\n",
            "epoch 216  sum of loss 4.5881947146321895\n",
            "epoch 217  sum of loss 11.826030698166171\n",
            "epoch 218  sum of loss 5.178511730087811\n",
            "epoch 219  sum of loss 13.377895752993426\n",
            "epoch 220  sum of loss 9.774937907182426\n",
            "epoch 221  sum of loss 8.970561941276635\n",
            "epoch 222  sum of loss 10.743835885835763\n",
            "epoch 223  sum of loss 6.266751034093321\n",
            "epoch 224  sum of loss 8.433174611607175\n",
            "epoch 225  sum of loss 10.135974422128305\n",
            "epoch 226  sum of loss 8.200769473124772\n",
            "epoch 227  sum of loss 9.769858592706615\n",
            "epoch 228  sum of loss 11.201824161311663\n",
            "epoch 229  sum of loss 10.796739580092241\n",
            "epoch 230  sum of loss 9.220003361914845\n",
            "epoch 231  sum of loss 13.302917078441624\n",
            "epoch 232  sum of loss 10.431298601193872\n",
            "epoch 233  sum of loss 10.651768843182762\n",
            "epoch 234  sum of loss 14.256399279836652\n",
            "epoch 235  sum of loss 13.391221737445292\n",
            "epoch 236  sum of loss 14.39272303685851\n",
            "epoch 237  sum of loss 12.326637518907336\n",
            "epoch 238  sum of loss 8.564680824231806\n",
            "epoch 239  sum of loss 14.291999505730695\n",
            "epoch 240  sum of loss 8.637373855137062\n",
            "epoch 241  sum of loss 10.960460894537968\n",
            "epoch 242  sum of loss 14.520766448466013\n",
            "epoch 243  sum of loss 9.536143834137224\n",
            "epoch 244  sum of loss 12.21095333231742\n",
            "epoch 245  sum of loss 8.545042686982523\n",
            "epoch 246  sum of loss 8.497452506507598\n",
            "epoch 247  sum of loss 10.56710476580103\n",
            "epoch 248  sum of loss 7.979895621586968\n",
            "epoch 249  sum of loss 8.335087659727389\n",
            "epoch 250  sum of loss 11.069386783437166\n",
            "epoch 251  sum of loss 9.949222563635132\n",
            "epoch 252  sum of loss 7.987713117786681\n",
            "epoch 253  sum of loss 7.703192463343237\n",
            "epoch 254  sum of loss 6.469813527865241\n",
            "epoch 255  sum of loss 7.750391861068122\n",
            "epoch 256  sum of loss 11.85490534700822\n",
            "epoch 257  sum of loss 7.228467611266827\n",
            "epoch 258  sum of loss 8.45107865217144\n",
            "epoch 259  sum of loss 12.100774907748947\n",
            "epoch 260  sum of loss 4.075395346747147\n",
            "epoch 261  sum of loss 10.833581217513547\n",
            "epoch 262  sum of loss 9.638191827020346\n",
            "epoch 263  sum of loss 8.65095233369968\n",
            "epoch 264  sum of loss 11.319408676649434\n",
            "epoch 265  sum of loss 14.560774238197824\n",
            "epoch 266  sum of loss 15.5258141423005\n",
            "epoch 267  sum of loss 13.493348331566267\n",
            "epoch 268  sum of loss 13.721670649828328\n",
            "epoch 269  sum of loss 11.605348991498191\n",
            "epoch 270  sum of loss 12.278355974400679\n",
            "epoch 271  sum of loss 9.392148435571354\n",
            "epoch 272  sum of loss 7.744221605689207\n",
            "epoch 273  sum of loss 19.43150453214976\n",
            "epoch 274  sum of loss 10.904568370287972\n",
            "epoch 275  sum of loss 15.48755906452952\n",
            "epoch 276  sum of loss 5.241686781435403\n",
            "epoch 277  sum of loss 7.532556305755735\n",
            "epoch 278  sum of loss 11.251544503291424\n",
            "epoch 279  sum of loss 5.570614042130307\n",
            "epoch 280  sum of loss 6.265041458098206\n",
            "epoch 281  sum of loss 11.929135297934621\n",
            "epoch 282  sum of loss 6.337637780370287\n",
            "epoch 283  sum of loss 13.547755858315565\n",
            "epoch 284  sum of loss 9.353339936966552\n",
            "epoch 285  sum of loss 10.169862891156468\n",
            "epoch 286  sum of loss 8.845826452782404\n",
            "epoch 287  sum of loss 9.555435701467207\n",
            "epoch 288  sum of loss 11.222389710625624\n",
            "epoch 289  sum of loss 7.054802206997103\n",
            "epoch 290  sum of loss 8.862070929025979\n",
            "epoch 291  sum of loss 9.622263802432183\n",
            "epoch 292  sum of loss 8.320137162358687\n",
            "epoch 293  sum of loss 7.291418939161969\n",
            "epoch 294  sum of loss 12.948932016373128\n",
            "epoch 295  sum of loss 12.077538772943251\n",
            "epoch 296  sum of loss 7.489958318846006\n",
            "epoch 297  sum of loss 9.629665462312364\n",
            "epoch 298  sum of loss 12.75110002642271\n",
            "epoch 299  sum of loss 9.733735035996446\n",
            "epoch 300  sum of loss 9.903038091133514\n",
            "epoch 301  sum of loss 10.090430061190306\n",
            "epoch 302  sum of loss 8.153429197144098\n",
            "epoch 303  sum of loss 8.097638094409714\n",
            "epoch 304  sum of loss 11.177736427227504\n",
            "epoch 305  sum of loss 8.636474000972395\n",
            "epoch 306  sum of loss 13.208578206927449\n",
            "epoch 307  sum of loss 8.80877857158088\n",
            "epoch 308  sum of loss 13.12980366363908\n",
            "epoch 309  sum of loss 5.378949210757435\n",
            "epoch 310  sum of loss 9.093094222743407\n",
            "epoch 311  sum of loss 7.436829488615164\n",
            "epoch 312  sum of loss 7.365930219216526\n",
            "epoch 313  sum of loss 9.383600045511733\n",
            "epoch 314  sum of loss 9.096590922332643\n",
            "epoch 315  sum of loss 8.072063088527447\n",
            "epoch 316  sum of loss 10.433633090274723\n",
            "epoch 317  sum of loss 8.933889890452793\n",
            "epoch 318  sum of loss 6.898083103023453\n",
            "epoch 319  sum of loss 6.71555022774269\n",
            "epoch 320  sum of loss 15.200205312173475\n",
            "epoch 321  sum of loss 13.684778058975223\n",
            "epoch 322  sum of loss 9.013921972877496\n",
            "epoch 323  sum of loss 4.836294378130537\n",
            "epoch 324  sum of loss 11.34818714736039\n",
            "epoch 325  sum of loss 11.363155407115187\n",
            "epoch 326  sum of loss 11.391911042715007\n",
            "epoch 327  sum of loss 16.565483414294295\n",
            "epoch 328  sum of loss 8.97320824172754\n",
            "epoch 329  sum of loss 4.980077604204683\n",
            "epoch 330  sum of loss 8.686950633296545\n",
            "epoch 331  sum of loss 9.501024007421453\n",
            "epoch 332  sum of loss 15.671219168667392\n",
            "epoch 333  sum of loss 8.236075010894456\n",
            "epoch 334  sum of loss 7.8667783440189005\n",
            "epoch 335  sum of loss 6.273946231900693\n",
            "epoch 336  sum of loss 5.414749391277155\n",
            "epoch 337  sum of loss 6.117811223099299\n",
            "epoch 338  sum of loss 11.385136915411929\n",
            "epoch 339  sum of loss 13.330400999063649\n",
            "epoch 340  sum of loss 6.5465618417652784\n",
            "epoch 341  sum of loss 9.335174860589392\n",
            "epoch 342  sum of loss 9.799826611780233\n",
            "epoch 343  sum of loss 10.001918334300084\n",
            "epoch 344  sum of loss 8.851902666446176\n",
            "epoch 345  sum of loss 11.134922376548765\n",
            "epoch 346  sum of loss 13.955796136571267\n",
            "epoch 347  sum of loss 9.730925106475993\n",
            "epoch 348  sum of loss 9.662407827697097\n",
            "epoch 349  sum of loss 9.238037076098397\n",
            "epoch 350  sum of loss 11.76227481993916\n",
            "epoch 351  sum of loss 8.194012142056021\n",
            "epoch 352  sum of loss 10.447428183885071\n",
            "epoch 353  sum of loss 8.00836877187\n",
            "epoch 354  sum of loss 9.897652839317372\n",
            "epoch 355  sum of loss 9.787705946552919\n",
            "epoch 356  sum of loss 10.800409599035033\n",
            "epoch 357  sum of loss 9.443015979476415\n",
            "epoch 358  sum of loss 9.42587885952469\n",
            "epoch 359  sum of loss 13.077578491938738\n",
            "epoch 360  sum of loss 11.59029874503318\n",
            "epoch 361  sum of loss 7.248806929097864\n",
            "epoch 362  sum of loss 9.508470178763892\n",
            "epoch 363  sum of loss 8.352458676129242\n",
            "epoch 364  sum of loss 10.25667920069241\n",
            "epoch 365  sum of loss 10.635749916294207\n",
            "epoch 366  sum of loss 11.088381648600118\n",
            "epoch 367  sum of loss 10.085823273718763\n",
            "epoch 368  sum of loss 12.239638296359441\n",
            "epoch 369  sum of loss 11.275406546451975\n",
            "epoch 370  sum of loss 8.161083004457112\n",
            "epoch 371  sum of loss 3.5752840251836226\n",
            "epoch 372  sum of loss 5.726169184255131\n",
            "epoch 373  sum of loss 11.648711329559635\n",
            "epoch 374  sum of loss 7.492622298828152\n",
            "epoch 375  sum of loss 5.486480427051401\n",
            "epoch 376  sum of loss 7.678438286538702\n",
            "epoch 377  sum of loss 10.413882010681268\n",
            "epoch 378  sum of loss 7.166392282626356\n",
            "epoch 379  sum of loss 8.023723577888354\n",
            "epoch 380  sum of loss 11.354268058203358\n",
            "epoch 381  sum of loss 12.973371901291479\n",
            "epoch 382  sum of loss 7.317012973553553\n",
            "epoch 383  sum of loss 6.609525569081319\n",
            "epoch 384  sum of loss 11.440121940654347\n",
            "epoch 385  sum of loss 5.169580031077121\n",
            "epoch 386  sum of loss 9.718614196668375\n",
            "epoch 387  sum of loss 9.083542680585353\n",
            "epoch 388  sum of loss 9.444625712059462\n",
            "epoch 389  sum of loss 8.165377847325555\n",
            "epoch 390  sum of loss 9.43981514077074\n",
            "epoch 391  sum of loss 7.8117380764667255\n",
            "epoch 392  sum of loss 8.05996932610589\n",
            "epoch 393  sum of loss 1.8165648430330255\n",
            "epoch 394  sum of loss 4.58833532957141\n",
            "epoch 395  sum of loss 8.761496484718316\n",
            "epoch 396  sum of loss 7.989056377134538\n",
            "epoch 397  sum of loss 9.97630535768829\n",
            "epoch 398  sum of loss 10.05824616300085\n",
            "epoch 399  sum of loss 13.854650142856023\n",
            "epoch 400  sum of loss 12.499223442154332\n",
            "epoch 401  sum of loss 10.4485009879682\n",
            "epoch 402  sum of loss 8.005732896157525\n",
            "epoch 403  sum of loss 8.59976854138312\n",
            "epoch 404  sum of loss 4.882575114505553\n",
            "epoch 405  sum of loss 6.869486484019994\n",
            "epoch 406  sum of loss 13.061749506692347\n",
            "epoch 407  sum of loss 7.0555822500509935\n",
            "epoch 408  sum of loss 8.008377497385755\n",
            "epoch 409  sum of loss 3.4393105827229546\n",
            "epoch 410  sum of loss 11.553125815237207\n",
            "epoch 411  sum of loss 7.029291897121917\n",
            "epoch 412  sum of loss 8.185901325916046\n",
            "epoch 413  sum of loss 7.833487469397985\n",
            "epoch 414  sum of loss 7.742817004018176\n",
            "epoch 415  sum of loss 7.427831940379695\n",
            "epoch 416  sum of loss 13.205200357775505\n",
            "epoch 417  sum of loss 6.961503758612281\n",
            "epoch 418  sum of loss 10.549378185508328\n",
            "epoch 419  sum of loss 7.20778296208477\n",
            "epoch 420  sum of loss 8.223464500323209\n",
            "epoch 421  sum of loss 13.510281160571516\n",
            "epoch 422  sum of loss 9.449551137598998\n",
            "epoch 423  sum of loss 5.983733774825381\n",
            "epoch 424  sum of loss 8.042130077317214\n",
            "epoch 425  sum of loss 10.54705627319799\n",
            "epoch 426  sum of loss 12.204938079880495\n",
            "epoch 427  sum of loss 12.54351456561001\n",
            "epoch 428  sum of loss 7.699214655756678\n",
            "epoch 429  sum of loss 8.216377255220024\n",
            "epoch 430  sum of loss 9.066190475243772\n",
            "epoch 431  sum of loss 8.050366445639666\n",
            "epoch 432  sum of loss 4.538558090390353\n",
            "epoch 433  sum of loss 7.852907947929398\n",
            "epoch 434  sum of loss 7.772735904799131\n",
            "epoch 435  sum of loss 6.078291550920104\n",
            "epoch 436  sum of loss 14.110261766374585\n",
            "epoch 437  sum of loss 7.168890085575299\n",
            "epoch 438  sum of loss 9.461843701549192\n",
            "epoch 439  sum of loss 9.295610919827489\n",
            "epoch 440  sum of loss 3.696235641345026\n",
            "epoch 441  sum of loss 11.097877672424085\n",
            "epoch 442  sum of loss 9.694278553945026\n",
            "epoch 443  sum of loss 12.022618067479474\n",
            "epoch 444  sum of loss 7.891836101338674\n",
            "epoch 445  sum of loss 7.920861349311277\n",
            "epoch 446  sum of loss 8.818732617555927\n",
            "epoch 447  sum of loss 6.50381948171582\n",
            "epoch 448  sum of loss 7.905993554234016\n",
            "epoch 449  sum of loss 4.630351769232078\n",
            "epoch 450  sum of loss 4.908096617190919\n",
            "epoch 451  sum of loss 5.895148513551067\n",
            "epoch 452  sum of loss 8.117734892401616\n",
            "epoch 453  sum of loss 7.307466747522964\n",
            "epoch 454  sum of loss 5.265049857000131\n",
            "epoch 455  sum of loss 2.3106683314856378\n",
            "epoch 456  sum of loss 10.904506660937546\n",
            "epoch 457  sum of loss 6.869526617851978\n",
            "epoch 458  sum of loss 10.128148642496962\n",
            "epoch 459  sum of loss 6.875641018418036\n",
            "epoch 460  sum of loss 8.619693654321525\n",
            "epoch 461  sum of loss 6.61177541902295\n",
            "epoch 462  sum of loss 6.733798886621157\n",
            "epoch 463  sum of loss 8.098767575141622\n",
            "epoch 464  sum of loss 8.72964554149946\n",
            "epoch 465  sum of loss 4.4656288332035246\n",
            "epoch 466  sum of loss 9.531438569902319\n",
            "epoch 467  sum of loss 11.904943817040209\n",
            "epoch 468  sum of loss 13.899301341453175\n",
            "epoch 469  sum of loss 4.009876472746803\n",
            "epoch 470  sum of loss 8.69941855556802\n",
            "epoch 471  sum of loss 9.152157699270916\n",
            "epoch 472  sum of loss 8.813888519285285\n",
            "epoch 473  sum of loss 7.830919827421217\n",
            "epoch 474  sum of loss 7.7307817641514625\n",
            "epoch 475  sum of loss 8.639401694227294\n",
            "epoch 476  sum of loss 6.378422401494187\n",
            "epoch 477  sum of loss 10.689360139699993\n",
            "epoch 478  sum of loss 7.755065624847178\n",
            "epoch 479  sum of loss 10.106342843939291\n",
            "epoch 480  sum of loss 8.093194761245858\n",
            "epoch 481  sum of loss 9.452276698852767\n",
            "epoch 482  sum of loss 8.826711222605539\n",
            "epoch 483  sum of loss 10.653522145306022\n",
            "epoch 484  sum of loss 10.655672682691623\n",
            "epoch 485  sum of loss 10.938172969328754\n",
            "epoch 486  sum of loss 11.453936136621413\n",
            "epoch 487  sum of loss 5.853261435524396\n",
            "epoch 488  sum of loss 6.991428062420482\n",
            "epoch 489  sum of loss 12.361609553051434\n",
            "epoch 490  sum of loss 7.978275629403094\n",
            "epoch 491  sum of loss 9.82576386120736\n",
            "epoch 492  sum of loss 5.352977122826656\n",
            "epoch 493  sum of loss 6.5681488125702305\n",
            "epoch 494  sum of loss 8.878107656406584\n",
            "epoch 495  sum of loss 9.85337993307105\n",
            "epoch 496  sum of loss 4.912850041717751\n",
            "epoch 497  sum of loss 6.864392529515452\n",
            "epoch 498  sum of loss 8.352792029945466\n",
            "epoch 499  sum of loss 7.295205904049854\n",
            "epoch 500  sum of loss 5.742685446625841\n",
            "epoch 501  sum of loss 6.116761026034044\n",
            "epoch 502  sum of loss 11.599083893418998\n",
            "epoch 503  sum of loss 9.062462798012334\n",
            "epoch 504  sum of loss 11.760672934301946\n",
            "epoch 505  sum of loss 4.288552943008013\n",
            "epoch 506  sum of loss 5.660814198390731\n",
            "epoch 507  sum of loss 6.3166128503477905\n",
            "epoch 508  sum of loss 8.468685174962255\n",
            "epoch 509  sum of loss 4.977434778611032\n",
            "epoch 510  sum of loss 6.8943631934371155\n",
            "epoch 511  sum of loss 6.984496777147881\n",
            "epoch 512  sum of loss 4.588733993952889\n",
            "epoch 513  sum of loss 6.204695412477851\n",
            "epoch 514  sum of loss 7.305566877241208\n",
            "epoch 515  sum of loss 7.103825135271791\n",
            "epoch 516  sum of loss 6.600596612039277\n",
            "epoch 517  sum of loss 11.003388725081255\n",
            "epoch 518  sum of loss 7.0448202087148815\n",
            "epoch 519  sum of loss 7.425028426582394\n",
            "epoch 520  sum of loss 4.801700987314138\n",
            "epoch 521  sum of loss 7.03595973283109\n",
            "epoch 522  sum of loss 7.5491642725065615\n",
            "epoch 523  sum of loss 10.730106239919115\n",
            "epoch 524  sum of loss 9.225165792115485\n",
            "epoch 525  sum of loss 9.841773631709316\n",
            "epoch 526  sum of loss 5.277334377016656\n",
            "epoch 527  sum of loss 7.173979627510236\n",
            "epoch 528  sum of loss 7.951412056718476\n",
            "epoch 529  sum of loss 8.72471505411059\n",
            "epoch 530  sum of loss 6.09780610527306\n",
            "epoch 531  sum of loss 8.974857832914964\n",
            "epoch 532  sum of loss 4.580396271756208\n",
            "epoch 533  sum of loss 5.129155202517444\n",
            "epoch 534  sum of loss 9.25820714735557\n",
            "epoch 535  sum of loss 4.392614290546713\n",
            "epoch 536  sum of loss 6.25208404331571\n",
            "epoch 537  sum of loss 2.629382135191212\n",
            "epoch 538  sum of loss 5.9509830850881285\n",
            "epoch 539  sum of loss 5.570048272177681\n",
            "epoch 540  sum of loss 6.087238366488319\n",
            "epoch 541  sum of loss 8.613391835140433\n",
            "epoch 542  sum of loss 8.565046260219503\n",
            "epoch 543  sum of loss 6.5431570159453925\n",
            "epoch 544  sum of loss 6.072229912610055\n",
            "epoch 545  sum of loss 7.107390385323834\n",
            "epoch 546  sum of loss 4.165784404160413\n",
            "epoch 547  sum of loss 7.414163255616513\n",
            "epoch 548  sum of loss 8.096547481963126\n",
            "epoch 549  sum of loss 9.828534724089904\n",
            "epoch 550  sum of loss 8.050372229124486\n",
            "epoch 551  sum of loss 7.7136321613211605\n",
            "epoch 552  sum of loss 5.728115571473135\n",
            "epoch 553  sum of loss 7.41916711551308\n",
            "epoch 554  sum of loss 4.8857776169702705\n",
            "epoch 555  sum of loss 5.421085501029381\n",
            "epoch 556  sum of loss 8.340934791849342\n",
            "epoch 557  sum of loss 6.803322559182339\n",
            "epoch 558  sum of loss 10.749695022289586\n",
            "epoch 559  sum of loss 8.090234358473143\n",
            "epoch 560  sum of loss 5.884079314056792\n",
            "epoch 561  sum of loss 6.922767273653309\n",
            "epoch 562  sum of loss 7.196507007135055\n",
            "epoch 563  sum of loss 10.287985744848598\n",
            "epoch 564  sum of loss 6.173295427563504\n",
            "epoch 565  sum of loss 10.031075626925613\n",
            "epoch 566  sum of loss 7.064062058208695\n",
            "epoch 567  sum of loss 10.258776886853685\n",
            "epoch 568  sum of loss 8.301785765407264\n",
            "epoch 569  sum of loss 8.770003534873812\n",
            "epoch 570  sum of loss 5.497394635199371\n",
            "epoch 571  sum of loss 11.605186848470456\n",
            "epoch 572  sum of loss 7.042113314074003\n",
            "epoch 573  sum of loss 7.879372844757307\n",
            "epoch 574  sum of loss 7.781879648855463\n",
            "epoch 575  sum of loss 6.995056454516558\n",
            "epoch 576  sum of loss 11.77545943810458\n",
            "epoch 577  sum of loss 8.74637133219733\n",
            "epoch 578  sum of loss 9.841199062795022\n",
            "epoch 579  sum of loss 8.070379413964062\n",
            "epoch 580  sum of loss 7.0182132405417805\n",
            "epoch 581  sum of loss 3.2369193496810396\n",
            "epoch 582  sum of loss 5.228476057292866\n",
            "epoch 583  sum of loss 4.573010629549334\n",
            "epoch 584  sum of loss 9.045898963434748\n",
            "epoch 585  sum of loss 7.607768784363147\n",
            "epoch 586  sum of loss 8.755784634391098\n",
            "epoch 587  sum of loss 5.557692430209496\n",
            "epoch 588  sum of loss 4.23783552640467\n",
            "epoch 589  sum of loss 7.271363929428211\n",
            "epoch 590  sum of loss 6.177066992025188\n",
            "epoch 591  sum of loss 4.916726065010017\n",
            "epoch 592  sum of loss 7.056038279321443\n",
            "epoch 593  sum of loss 8.718896478614033\n",
            "epoch 594  sum of loss 8.726691772997608\n",
            "epoch 595  sum of loss 11.831263139039347\n",
            "epoch 596  sum of loss 5.7494200392432875\n",
            "epoch 597  sum of loss 7.896872318365603\n",
            "epoch 598  sum of loss 5.712463816507998\n",
            "epoch 599  sum of loss 4.9533966148103135\n",
            "epoch 600  sum of loss 6.50721023920944\n",
            "epoch 601  sum of loss 7.015919366513701\n",
            "epoch 602  sum of loss 4.853739209505958\n",
            "epoch 603  sum of loss 7.2608092299432085\n",
            "epoch 604  sum of loss 13.900756343876553\n",
            "epoch 605  sum of loss 4.8456581045738885\n",
            "epoch 606  sum of loss 7.196814566448782\n",
            "epoch 607  sum of loss 4.474099357809409\n",
            "epoch 608  sum of loss 6.085042647432787\n",
            "epoch 609  sum of loss 8.55422362249826\n",
            "epoch 610  sum of loss 8.193172952683085\n",
            "epoch 611  sum of loss 3.3994238218523294\n",
            "epoch 612  sum of loss 8.007432415823088\n",
            "epoch 613  sum of loss 4.826162751029603\n",
            "epoch 614  sum of loss 11.313337052610406\n",
            "epoch 615  sum of loss 7.942990763032334\n",
            "epoch 616  sum of loss 7.982636220454539\n",
            "epoch 617  sum of loss 4.260370834143192\n",
            "epoch 618  sum of loss 10.183863437417598\n",
            "epoch 619  sum of loss 9.723187294496213\n",
            "epoch 620  sum of loss 7.196772099094574\n",
            "epoch 621  sum of loss 6.9792546889628815\n",
            "epoch 622  sum of loss 4.408715170486434\n",
            "epoch 623  sum of loss 4.8862914273506775\n",
            "epoch 624  sum of loss 8.162585941871447\n",
            "epoch 625  sum of loss 6.146360441700445\n",
            "epoch 626  sum of loss 6.687986331592809\n",
            "epoch 627  sum of loss 6.847469526693087\n",
            "epoch 628  sum of loss 3.5888546094295917\n",
            "epoch 629  sum of loss 4.850141344786184\n",
            "epoch 630  sum of loss 4.68270202624181\n",
            "epoch 631  sum of loss 7.358789869642608\n",
            "epoch 632  sum of loss 7.896049654347523\n",
            "epoch 633  sum of loss 6.438376706885995\n",
            "epoch 634  sum of loss 7.004161954101978\n",
            "epoch 635  sum of loss 6.135487480002622\n",
            "epoch 636  sum of loss 4.5779329456649105\n",
            "epoch 637  sum of loss 5.115738597946347\n",
            "epoch 638  sum of loss 7.167541659561426\n",
            "epoch 639  sum of loss 7.192761263202034\n",
            "epoch 640  sum of loss 9.434284221204516\n",
            "epoch 641  sum of loss 6.726205938226418\n",
            "epoch 642  sum of loss 4.473226044239595\n",
            "epoch 643  sum of loss 11.605032525173671\n",
            "epoch 644  sum of loss 4.5522116283151375\n",
            "epoch 645  sum of loss 9.91013509598533\n",
            "epoch 646  sum of loss 10.849187426268365\n",
            "epoch 647  sum of loss 12.316112812712142\n",
            "epoch 648  sum of loss 12.654776107540872\n",
            "epoch 649  sum of loss 15.148468308185375\n",
            "epoch 650  sum of loss 5.574672031081159\n",
            "epoch 651  sum of loss 12.055114041846174\n",
            "epoch 652  sum of loss 5.688385173144024\n",
            "epoch 653  sum of loss 4.499972884422112\n",
            "epoch 654  sum of loss 7.484179795057016\n",
            "epoch 655  sum of loss 4.738625574980583\n",
            "epoch 656  sum of loss 6.690675106124839\n",
            "epoch 657  sum of loss 6.9244362806165025\n",
            "epoch 658  sum of loss 7.040985895941652\n",
            "epoch 659  sum of loss 1.429867876898295\n",
            "epoch 660  sum of loss 7.709428196154658\n",
            "epoch 661  sum of loss 9.110347170018336\n",
            "epoch 662  sum of loss 4.162367268183973\n",
            "epoch 663  sum of loss 9.307396026137608\n",
            "epoch 664  sum of loss 5.42639343846593\n",
            "epoch 665  sum of loss 12.009921781620848\n",
            "epoch 666  sum of loss 8.577896549425413\n",
            "epoch 667  sum of loss 3.458152255872992\n",
            "epoch 668  sum of loss 7.312191812668093\n",
            "epoch 669  sum of loss 4.219600589689087\n",
            "epoch 670  sum of loss 10.137888574444494\n",
            "epoch 671  sum of loss 5.3020022973880625\n",
            "epoch 672  sum of loss 6.568711355581764\n",
            "epoch 673  sum of loss 5.389619124471371\n",
            "epoch 674  sum of loss 5.628349788761252\n",
            "epoch 675  sum of loss 8.075791763196948\n",
            "epoch 676  sum of loss 8.87666271704741\n",
            "epoch 677  sum of loss 5.898819288025205\n",
            "epoch 678  sum of loss 5.9938477057376405\n",
            "epoch 679  sum of loss 8.20123644413451\n",
            "epoch 680  sum of loss 8.924969033181029\n",
            "epoch 681  sum of loss 8.258558610635344\n",
            "epoch 682  sum of loss 8.383036822314835\n",
            "epoch 683  sum of loss 10.45820373239131\n",
            "epoch 684  sum of loss 7.914803237169679\n",
            "epoch 685  sum of loss 6.961856746257943\n",
            "epoch 686  sum of loss 6.385445240177624\n",
            "epoch 687  sum of loss 12.258888790959057\n",
            "epoch 688  sum of loss 6.092617049319647\n",
            "epoch 689  sum of loss 7.548366647891692\n",
            "epoch 690  sum of loss 11.400557912593916\n",
            "epoch 691  sum of loss 8.344070123883432\n",
            "epoch 692  sum of loss 5.217580343504651\n",
            "epoch 693  sum of loss 8.836287785058204\n",
            "epoch 694  sum of loss 9.714316129683292\n",
            "epoch 695  sum of loss 11.797371169247171\n",
            "epoch 696  sum of loss 8.2549124136297\n",
            "epoch 697  sum of loss 4.657650196383415\n",
            "epoch 698  sum of loss 10.56344693323469\n",
            "epoch 699  sum of loss 8.322905668831405\n",
            "epoch 700  sum of loss 5.903477596511625\n",
            "epoch 701  sum of loss 9.070988971765573\n",
            "epoch 702  sum of loss 7.366137130058002\n",
            "epoch 703  sum of loss 8.27063756270238\n",
            "epoch 704  sum of loss 5.416667620269678\n",
            "epoch 705  sum of loss 8.67999078924249\n",
            "epoch 706  sum of loss 11.516663771926678\n",
            "epoch 707  sum of loss 6.615681532184158\n",
            "epoch 708  sum of loss 5.13998060777841\n",
            "epoch 709  sum of loss 2.3131862699698895\n",
            "epoch 710  sum of loss 13.16235698885187\n",
            "epoch 711  sum of loss 2.5727126972540497\n",
            "epoch 712  sum of loss 7.9999687159774435\n",
            "epoch 713  sum of loss 9.265260041963082\n",
            "epoch 714  sum of loss 5.337925041991397\n",
            "epoch 715  sum of loss 9.072074714178399\n",
            "epoch 716  sum of loss 8.630071982556952\n",
            "epoch 717  sum of loss 7.434387090078095\n",
            "epoch 718  sum of loss 2.4730938720322952\n",
            "epoch 719  sum of loss 6.871903433065399\n",
            "epoch 720  sum of loss 5.738460064795974\n",
            "epoch 721  sum of loss 8.918096929044955\n",
            "epoch 722  sum of loss 1.7072685798807843\n",
            "epoch 723  sum of loss 4.157303886271789\n",
            "epoch 724  sum of loss 5.542496392808019\n",
            "epoch 725  sum of loss 7.872707214484238\n",
            "epoch 726  sum of loss 8.28872966334185\n",
            "epoch 727  sum of loss 2.9327388261528684\n",
            "epoch 728  sum of loss 6.864087752020112\n",
            "epoch 729  sum of loss 5.614818446697787\n",
            "epoch 730  sum of loss 8.35555571520434\n",
            "epoch 731  sum of loss 8.035405575749182\n",
            "epoch 732  sum of loss 6.946021270735899\n",
            "epoch 733  sum of loss 7.910891720985722\n",
            "epoch 734  sum of loss 5.211286343459018\n",
            "epoch 735  sum of loss 6.737240858007833\n",
            "epoch 736  sum of loss 3.6858980337779905\n",
            "epoch 737  sum of loss 7.289158493793015\n",
            "epoch 738  sum of loss 2.778888057520362\n",
            "epoch 739  sum of loss 8.109319606355136\n",
            "epoch 740  sum of loss 3.783325766082323\n",
            "epoch 741  sum of loss 2.2829886815522067\n",
            "epoch 742  sum of loss 6.466831154302101\n",
            "epoch 743  sum of loss 4.658209909627025\n",
            "epoch 744  sum of loss 6.367172185997196\n",
            "epoch 745  sum of loss 5.720236681896649\n",
            "epoch 746  sum of loss 6.834686448989731\n",
            "epoch 747  sum of loss 7.310873332041457\n",
            "epoch 748  sum of loss 7.766822299521545\n",
            "epoch 749  sum of loss 5.4960844273261165\n",
            "epoch 750  sum of loss 2.9049046847000835\n",
            "epoch 751  sum of loss 5.426394087092418\n",
            "epoch 752  sum of loss 4.734618763829209\n",
            "epoch 753  sum of loss 7.408719124582398\n",
            "epoch 754  sum of loss 7.662087981221619\n",
            "epoch 755  sum of loss 4.635301248044371\n",
            "epoch 756  sum of loss 3.510257244189642\n",
            "epoch 757  sum of loss 3.8015009421276913\n",
            "epoch 758  sum of loss 7.9741351336256425\n",
            "epoch 759  sum of loss 6.146321360103936\n",
            "epoch 760  sum of loss 3.340298304220213\n",
            "epoch 761  sum of loss 2.4895736457359585\n",
            "epoch 762  sum of loss 6.622777801826002\n",
            "epoch 763  sum of loss 9.667626156853903\n",
            "epoch 764  sum of loss 5.9333027764898985\n",
            "epoch 765  sum of loss 9.700096109228612\n",
            "epoch 766  sum of loss 7.962780446841664\n",
            "epoch 767  sum of loss 6.2453558354299155\n",
            "epoch 768  sum of loss 10.989362833374756\n",
            "epoch 769  sum of loss 7.105160865089797\n",
            "epoch 770  sum of loss 8.50907266150597\n",
            "epoch 771  sum of loss 6.5472411159771715\n",
            "epoch 772  sum of loss 4.949648090578075\n",
            "epoch 773  sum of loss 4.429336954972547\n",
            "epoch 774  sum of loss 5.264392515611373\n",
            "epoch 775  sum of loss 1.7222618916583023\n",
            "epoch 776  sum of loss 8.015986252499628\n",
            "epoch 777  sum of loss 12.075139022177689\n",
            "epoch 778  sum of loss 6.8973234450790954\n",
            "epoch 779  sum of loss 11.665425269987406\n",
            "epoch 780  sum of loss 8.608959950334665\n",
            "epoch 781  sum of loss 4.9558336894919055\n",
            "epoch 782  sum of loss 8.934697888486156\n",
            "epoch 783  sum of loss 4.165790112093768\n",
            "epoch 784  sum of loss 7.869502500331701\n",
            "epoch 785  sum of loss 4.937631472563452\n",
            "epoch 786  sum of loss 8.827357316908197\n",
            "epoch 787  sum of loss 7.113827576017117\n",
            "epoch 788  sum of loss 5.167204826925221\n",
            "epoch 789  sum of loss 8.056575494865633\n",
            "epoch 790  sum of loss 6.016453154420317\n",
            "epoch 791  sum of loss 6.337877617982111\n",
            "epoch 792  sum of loss 5.770485864001983\n",
            "epoch 793  sum of loss 5.442852390471511\n",
            "epoch 794  sum of loss 8.410854200161744\n",
            "epoch 795  sum of loss 9.952442947394935\n",
            "epoch 796  sum of loss 5.181618892308423\n",
            "epoch 797  sum of loss 7.256686436770287\n",
            "epoch 798  sum of loss 8.56353166383909\n",
            "epoch 799  sum of loss 7.059968814106903\n",
            "epoch 800  sum of loss 4.647396576827298\n",
            "epoch 801  sum of loss 8.857001350781545\n",
            "epoch 802  sum of loss 6.051306909389096\n",
            "epoch 803  sum of loss 7.7115852471565765\n",
            "epoch 804  sum of loss 8.072360007796924\n",
            "epoch 805  sum of loss 8.509835691029036\n",
            "epoch 806  sum of loss 11.957708246645042\n",
            "epoch 807  sum of loss 9.0729493070105\n",
            "epoch 808  sum of loss 4.267899819817211\n",
            "epoch 809  sum of loss 3.838792258884593\n",
            "epoch 810  sum of loss 6.251030115905367\n",
            "epoch 811  sum of loss 3.9552613682832987\n",
            "epoch 812  sum of loss 8.106026560140704\n",
            "epoch 813  sum of loss 8.335731219768164\n",
            "epoch 814  sum of loss 7.732968756813543\n",
            "epoch 815  sum of loss 8.45543764075541\n",
            "epoch 816  sum of loss 4.897576544760041\n",
            "epoch 817  sum of loss 3.754467869236306\n",
            "epoch 818  sum of loss 9.981530648363027\n",
            "epoch 819  sum of loss 9.017615573976638\n",
            "epoch 820  sum of loss 8.91785496010446\n",
            "epoch 821  sum of loss 9.229497001055751\n",
            "epoch 822  sum of loss 8.35388951950774\n",
            "epoch 823  sum of loss 1.8188326913267503\n",
            "epoch 824  sum of loss 3.596339603838131\n",
            "epoch 825  sum of loss 4.4910197293435274\n",
            "epoch 826  sum of loss 7.466054160133451\n",
            "epoch 827  sum of loss 5.054264489690426\n",
            "epoch 828  sum of loss 6.136981931943897\n",
            "epoch 829  sum of loss 6.949708002967874\n",
            "epoch 830  sum of loss 8.721231124887257\n",
            "epoch 831  sum of loss 5.187081081832947\n",
            "epoch 832  sum of loss 9.150446023538851\n",
            "epoch 833  sum of loss 4.855577150685711\n",
            "epoch 834  sum of loss 3.622135943751351\n",
            "epoch 835  sum of loss 4.922170075776499\n",
            "epoch 836  sum of loss 7.054725536849649\n",
            "epoch 837  sum of loss 5.635025170165859\n",
            "epoch 838  sum of loss 4.9986437818048675\n",
            "epoch 839  sum of loss 5.7547554882499625\n",
            "epoch 840  sum of loss 8.39535725301732\n",
            "epoch 841  sum of loss 7.326852457432672\n",
            "epoch 842  sum of loss 5.255875861709244\n",
            "epoch 843  sum of loss 6.175910238316294\n",
            "epoch 844  sum of loss 3.7467688247188726\n",
            "epoch 845  sum of loss 5.311831290273712\n",
            "epoch 846  sum of loss 7.049830244241617\n",
            "epoch 847  sum of loss 9.87774927454204\n",
            "epoch 848  sum of loss 10.816340615882563\n",
            "epoch 849  sum of loss 11.215754735728062\n",
            "epoch 850  sum of loss 9.23403969687654\n",
            "epoch 851  sum of loss 5.709607124107137\n",
            "epoch 852  sum of loss 10.004792246531382\n",
            "epoch 853  sum of loss 4.050663855383647\n",
            "epoch 854  sum of loss 6.893627907632512\n",
            "epoch 855  sum of loss 5.5294130679306885\n",
            "epoch 856  sum of loss 5.808935810037477\n",
            "epoch 857  sum of loss 3.650143706325703\n",
            "epoch 858  sum of loss 4.906103023853124\n",
            "epoch 859  sum of loss 6.497491134149132\n",
            "epoch 860  sum of loss 8.387423291448927\n",
            "epoch 861  sum of loss 4.0786824713531145\n",
            "epoch 862  sum of loss 4.406570119140279\n",
            "epoch 863  sum of loss 5.622402842904814\n",
            "epoch 864  sum of loss 7.315111315618425\n",
            "epoch 865  sum of loss 12.076737057287827\n",
            "epoch 866  sum of loss 9.233447693386436\n",
            "epoch 867  sum of loss 7.75451979186414\n",
            "epoch 868  sum of loss 4.657601800391081\n",
            "epoch 869  sum of loss 9.414702474473094\n",
            "epoch 870  sum of loss 8.154758978688717\n",
            "epoch 871  sum of loss 11.371182567490708\n",
            "epoch 872  sum of loss 3.361986254220159\n",
            "epoch 873  sum of loss 7.442996396062978\n",
            "epoch 874  sum of loss 5.499529287398696\n",
            "epoch 875  sum of loss 6.311070876004148\n",
            "epoch 876  sum of loss 5.872043582859113\n",
            "epoch 877  sum of loss 6.945366212955393\n",
            "epoch 878  sum of loss 6.691262388515777\n",
            "epoch 879  sum of loss 8.838818232382293\n",
            "epoch 880  sum of loss 10.149079442669464\n",
            "epoch 881  sum of loss 6.422874802228975\n",
            "epoch 882  sum of loss 4.822940551897583\n",
            "epoch 883  sum of loss 3.8892564570284125\n",
            "epoch 884  sum of loss 5.486491643397205\n",
            "epoch 885  sum of loss 2.0594412192776987\n",
            "epoch 886  sum of loss 4.351038232753144\n",
            "epoch 887  sum of loss 7.2909700526215\n",
            "epoch 888  sum of loss 6.685949016116648\n",
            "epoch 889  sum of loss 4.320540956085431\n",
            "epoch 890  sum of loss 7.232196625406523\n",
            "epoch 891  sum of loss 8.996865120156729\n",
            "epoch 892  sum of loss 7.972494791387648\n",
            "epoch 893  sum of loss 9.960694375424842\n",
            "epoch 894  sum of loss 8.351593602301813\n",
            "epoch 895  sum of loss 6.3004459615354715\n",
            "epoch 896  sum of loss 7.410766380876125\n",
            "epoch 897  sum of loss 7.636597400141526\n",
            "epoch 898  sum of loss 5.99938143709267\n",
            "epoch 899  sum of loss 5.9399543892675\n",
            "epoch 900  sum of loss 7.7098176467473\n",
            "epoch 901  sum of loss 5.812904648561629\n",
            "epoch 902  sum of loss 7.167201803646552\n",
            "epoch 903  sum of loss 4.31175562355457\n",
            "epoch 904  sum of loss 10.25900350679314\n",
            "epoch 905  sum of loss 6.0492941741284945\n",
            "epoch 906  sum of loss 15.040399746189664\n",
            "epoch 907  sum of loss 5.742440470261416\n",
            "epoch 908  sum of loss 9.664076124260921\n",
            "epoch 909  sum of loss 10.295528824238001\n",
            "epoch 910  sum of loss 4.63592490231129\n",
            "epoch 911  sum of loss 3.396380233169294\n",
            "epoch 912  sum of loss 7.440878732819573\n",
            "epoch 913  sum of loss 6.125661273805867\n",
            "epoch 914  sum of loss 13.12842801041832\n",
            "epoch 915  sum of loss 7.846720500030148\n",
            "epoch 916  sum of loss 7.190409960088018\n",
            "epoch 917  sum of loss 7.291657946177032\n",
            "epoch 918  sum of loss 6.394143206642184\n",
            "epoch 919  sum of loss 9.416253705742411\n",
            "epoch 920  sum of loss 8.19022681383284\n",
            "epoch 921  sum of loss 9.01336174835534\n",
            "epoch 922  sum of loss 7.471387706844952\n",
            "epoch 923  sum of loss 6.3040324057653185\n",
            "epoch 924  sum of loss 3.8414480665038004\n",
            "epoch 925  sum of loss 6.739505742685152\n",
            "epoch 926  sum of loss 11.557224851689423\n",
            "epoch 927  sum of loss 5.2664307738119875\n",
            "epoch 928  sum of loss 4.5368964955408515\n",
            "epoch 929  sum of loss 6.71033337828242\n",
            "epoch 930  sum of loss 2.1921018384116557\n",
            "epoch 931  sum of loss 9.597926144711009\n",
            "epoch 932  sum of loss 2.9018652613226057\n",
            "epoch 933  sum of loss 5.478719868205787\n",
            "epoch 934  sum of loss 5.650899887041681\n",
            "epoch 935  sum of loss 8.01058237130844\n",
            "epoch 936  sum of loss 5.153868094726369\n",
            "epoch 937  sum of loss 6.968491882649925\n",
            "epoch 938  sum of loss 8.381692275208477\n",
            "epoch 939  sum of loss 5.955824197858952\n",
            "epoch 940  sum of loss 5.818508156051208\n",
            "epoch 941  sum of loss 9.79485913069274\n",
            "epoch 942  sum of loss 4.177982654590001\n",
            "epoch 943  sum of loss 7.549056440449072\n",
            "epoch 944  sum of loss 7.2032380984049205\n",
            "epoch 945  sum of loss 6.547510068697928\n",
            "epoch 946  sum of loss 6.809664346335687\n",
            "epoch 947  sum of loss 8.121189861766451\n",
            "epoch 948  sum of loss 9.17887200676501\n",
            "epoch 949  sum of loss 2.8592145792819235\n",
            "epoch 950  sum of loss 6.8516016380991\n",
            "epoch 951  sum of loss 6.497181881437622\n",
            "epoch 952  sum of loss 5.824935492004132\n",
            "epoch 953  sum of loss 6.9053481723809815\n",
            "epoch 954  sum of loss 5.674843398997996\n",
            "epoch 955  sum of loss 12.83411497428845\n",
            "epoch 956  sum of loss 7.221656487102739\n",
            "epoch 957  sum of loss 7.678672170624982\n",
            "epoch 958  sum of loss 5.993111441989413\n",
            "epoch 959  sum of loss 5.694832489971314\n",
            "epoch 960  sum of loss 7.590044729866767\n",
            "epoch 961  sum of loss 14.139126779350203\n",
            "epoch 962  sum of loss 5.14464499579945\n",
            "epoch 963  sum of loss 4.872386732295589\n",
            "epoch 964  sum of loss 4.984935747202851\n",
            "epoch 965  sum of loss 9.812673459075532\n",
            "epoch 966  sum of loss 7.9195613523060775\n",
            "epoch 967  sum of loss 6.5839078401468525\n",
            "epoch 968  sum of loss 7.695507227401283\n",
            "epoch 969  sum of loss 4.952439432040572\n",
            "epoch 970  sum of loss 8.845097464171348\n",
            "epoch 971  sum of loss 6.894374395976344\n",
            "epoch 972  sum of loss 6.881206414301429\n",
            "epoch 973  sum of loss 9.749405695627686\n",
            "epoch 974  sum of loss 4.579363509827716\n",
            "epoch 975  sum of loss 5.282994710241894\n",
            "epoch 976  sum of loss 11.540852847314504\n",
            "epoch 977  sum of loss 4.798032341518014\n",
            "epoch 978  sum of loss 10.145749409814815\n",
            "epoch 979  sum of loss 8.527485715355871\n",
            "epoch 980  sum of loss 7.8890745438339405\n",
            "epoch 981  sum of loss 4.284260385923178\n",
            "epoch 982  sum of loss 6.382659506799764\n",
            "epoch 983  sum of loss 5.203540160775263\n",
            "epoch 984  sum of loss 2.131668612812427\n",
            "epoch 985  sum of loss 6.903343361635754\n",
            "epoch 986  sum of loss 10.01679449074177\n",
            "epoch 987  sum of loss 7.750068458404171\n",
            "epoch 988  sum of loss 8.587681988518522\n",
            "epoch 989  sum of loss 8.669326465392416\n",
            "epoch 990  sum of loss 5.827209004057453\n",
            "epoch 991  sum of loss 7.784131001671199\n",
            "epoch 992  sum of loss 7.0166058328476675\n",
            "epoch 993  sum of loss 3.6745553200621415\n",
            "epoch 994  sum of loss 10.006957788089741\n",
            "epoch 995  sum of loss 7.8342708601669075\n",
            "epoch 996  sum of loss 5.388624835193429\n",
            "epoch 997  sum of loss 6.062731535054756\n",
            "epoch 998  sum of loss 3.865411053514153\n",
            "epoch 999  sum of loss 6.915993191542724\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gcnn.train()\n",
        "hist=[]\n",
        "j=0\n",
        "\n",
        "for k in range(1000):\n",
        "  loss1=0\n",
        "  ids=[random.randint(0,3500) for _ in range(128)]\n",
        "  for i in ids:\n",
        "    pr1=Data(x=protein_embed_1[i],edge_index=protein_1[i],batch=batch_1[i])\n",
        "    pr2=Data(x=protein_embed_2[i],edge_index=protein_2[i],batch=batch_2[i])\n",
        "    gcnn_optimizer.zero_grad()\n",
        "    output = gcnn(pr1,pr2)\n",
        "    loss = gcnn_criterion(output, torch.tensor([labels[i]]).float())\n",
        "    loss.backward()\n",
        "    gcnn_optimizer.step()\n",
        "    loss1+=loss.item()\n",
        "  print(f\"epoch {k}  sum of loss {loss1}\")\n",
        "  hist.append(loss1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(hist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "P8viHqpn94dw",
        "outputId": "33042de3-643a-47b8-eff5-e6edb3062330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e4ffcc5cbe0>]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACBH0lEQVR4nO2dd5wU5f3HP7N7d3t3XOOAu6Pc0QWkiTQRVFQU0SjW2FtMjBG70Wgs0ZgEY4qaRE1MIv4SxZYoKlEUkSIKCEhHOkg9+vW+O78/7nbvmdnpO1vu9vN+ve7F7swzzzw7u8zzmW97JFmWZRBCCCGExAhPvAdACCGEkOSC4oMQQgghMYXigxBCCCExheKDEEIIITGF4oMQQgghMYXigxBCCCExheKDEEIIITGF4oMQQgghMSUl3gNQEwgEsH//fmRnZ0OSpHgPhxBCCCEWkGUZlZWV6NatGzweY9tGwomP/fv3o7i4ON7DIIQQQogD9uzZgx49ehi2STjxkZ2dDaB58Dk5OXEeDSGEEEKsUFFRgeLi4tA8bkTCiY+gqyUnJ4figxBCCGljWAmZYMApIYQQQmIKxQchhBBCYgrFByGEEEJiCsUHIYQQQmKKLfHx0ksvYdiwYaFg0HHjxuHjjz8O7a+rq8O0adPQqVMnZGVl4bLLLsPBgwddHzQhhBBC2i62xEePHj3w9NNPY+XKlVixYgXOOussTJ06FRs2bAAA3Hvvvfjwww/xzjvvYOHChdi/fz8uvfTSqAycEEIIIW0TSZZlOZIO8vPz8bvf/Q6XX345unTpgpkzZ+Lyyy8HAGzatAmDBg3CkiVLcMopp1jqr6KiArm5uSgvL2eqLSGEENJGsDN/O4758Pv9ePPNN1FdXY1x48Zh5cqVaGxsxKRJk0JtBg4ciJKSEixZskS3n/r6elRUVCj+CCGEENJ+sS0+1q1bh6ysLPh8Ptx222147733cOKJJ6K0tBRpaWnIy8tTtC8sLERpaaluf9OnT0dubm7oj6XVCSGEkPaNbfExYMAArF69GsuWLcNPfvIT3Hjjjdi4caPjATz88MMoLy8P/e3Zs8dxX4QQQghJfGyXV09LS0O/fv0AACNHjsTy5cvx/PPP48orr0RDQwPKysoU1o+DBw+iqKhItz+fzwefz2d/5IQQQghpk0Rc5yMQCKC+vh4jR45Eamoq5s2bF9q3efNm7N69G+PGjYv0NIQQQghpJ9iyfDz88MOYMmUKSkpKUFlZiZkzZ2LBggX45JNPkJubi1tuuQX33Xcf8vPzkZOTgzvvvBPjxo2znOkSKyrrGjFz2W6cP7QrivMz4z0cQgghJKmwJT4OHTqEG264AQcOHEBubi6GDRuGTz75BOeccw4A4Nlnn4XH48Fll12G+vp6TJ48GS+++GJUBh4JT3ywEf/9Zi9eWrgdqx8/N97DIYQQQpKKiOt8uE0s6nycOn0e9pfXAQB2PX1BVM5BCCGEJBMxqfNBCCGEEOKEpBQfkiTFewiEEEJI0pKk4iPeIyCEEEKSF4oPQgghhMSU5BQfoPoghBBC4kVSig9CCCGExI+kFB8eGj4IIYSQuJGU4oPZLoQQQkj8SErxQQghhJD4kZTig4YPQgghJH4kp/iI9wAIIYSQJCY5xQdNH4QQQkjcSE7xEe8BEEIIIUlMUooPQgghhMSPpBQfHrpdCCGEkLiRlOKD2oMQQgiJH0kpPgghhBASPyg+CCGEEBJTklJ8MNWWEEIIiR/JKT7iPQBCCCEkiUlO8UH1QQghhMQNig9CCCGExJSkFB+s80EIIYTEj6QUH5QehBBCSPxISvFBCCGEkPiRnOKDbhdCCCEkbiSl+KD0IIQQQuJHUomPnUeq8Y8vdqC+KRDadspv5mHt3rL4DYoQQghJMlLiPYBYcubvF4RtK62ow0/fWYNP7z0j9gMihBBCkpCksnzoIdERQwghhMQMig8AHXzeeA+BEEIISRooPgBkpafGewiEEEJI0kDxASCLlg9CCCEkZlB8AOiQllRxt4QQQkhcofgA0MFH8UEIIYTECooPQgghhMQUig8ATYHWomOyLOPP87Zi0ZbDcRwRIYQQ0n6hvwGAPyCHXn+8vhR/mLsFALDr6QviNSRCCCGk3ULLB4Amf6v42HG4Ko4jIYQQQto/FB9QWj7EdV8IIYQQ4j4UHwCaBPFR1+iP40gIIYSQ9g/FB5SWj7pGWj4IIYSQaELxAaDR3yo46pto+SCEEEKiSdKID1mWdfcx5oMQQgiJHUkjPsS4DqN9jPkghBBCokvyiA+/fcuHPyBj1e7jCrcMIYQQQiIjacRHY0BfQIgVTkXLxzNzNuGSF7/Cz99dF9WxEUIIIclE8ogPg1gOPcvH3xbtAAC8s3Jv9AZGCCGEJBlJIz6MYj4aBZdMAwNOCSGEkKhiS3xMnz4do0ePRnZ2NgoKCnDxxRdj8+bNijYTJ06EJEmKv9tuu83VQTvBKG5DtHz4DUQKIYQQQiLHlvhYuHAhpk2bhqVLl2Lu3LlobGzEueeei+rqakW7H/3oRzhw4EDo75lnnnF10E4wCjhtiqP4qG3wY9Xu44apwIQQQkh7wtaqtnPmzFG8f/XVV1FQUICVK1fi9NNPD23PzMxEUVGROyN0iSaDgFO/sM8fYxFw7T+W4pvdZfj1JUNw7dieMT03IYQQEg8iivkoLy8HAOTn5yu2v/766+jcuTOGDBmChx9+GDU1Nbp91NfXo6KiQvEXDXp26oAvHjxTc1/Q8rHtUCV2HK7WbBMtvtldBgB4a/memJ6XEEIIiRe2LB8igUAA99xzD8aPH48hQ4aEtl9zzTXo2bMnunXrhrVr1+JnP/sZNm/ejHfffVezn+nTp+PJJ590OgzLpHo9KM7PRL+CLGw7VKXY5w/IaPQHMOmPiyI+T32THx+vK8X4fp3RJdtn+Tgp4jMTQgghbQPH4mPatGlYv349Fi9erNh+6623hl4PHToUXbt2xdlnn43t27ejb9++Yf08/PDDuO+++0LvKyoqUFxc7HRYpuSkh3/kJr+MmgZ3Kps+/9lWvLhgO0ryM7FIx9JCCCGEJDOOxMcdd9yB2bNnY9GiRejRo4dh27FjxwIAtm3bpik+fD4ffD7rFoJI8aV4w7Y1BQIIuBRoOmdDKQBg9zF9VxMAHKtuwGtLv2vdINH2QQghJDmwJT5kWcadd96J9957DwsWLEDv3r1Nj1m9ejUAoGvXro4G6Da+1PAwl/qmgGuBph6LIuLet1Zj4ZbDofeUHoQQQpIFW+Jj2rRpmDlzJt5//31kZ2ejtLT5KT83NxcZGRnYvn07Zs6cifPPPx+dOnXC2rVrce+99+L000/HsGHDovIB7JKuYfmobfC7ZvnwWhQfX20/onhPwwchhJBkwZb4eOmllwA0FxITmTFjBm666SakpaXhs88+w3PPPYfq6moUFxfjsssuw6OPPuragCNFz/LR6JL4sCoiJEkCwNoehBBCkg/bbhcjiouLsXDhwogGFG18KdrZxdX1Ta70L1lUHx5VMxo+CCGEJAtJs7ZLEK2AUwCorHNHfKhFhR5q94xV0UIIIYS0dZJOfKRruF0AoMoly4fXovrwWFUphBBCSDsj6cSHnuWjyiXLh3W3i8ry4crZCSGEkMQnCcWHnuWj0ZX+rYoIGj4IIYQkK0knPtJTEyTmw6OO+XDl9IQQQkjCk3TiQyvVFnBTfDhzuxBCCCHJQvKJD123i0viw6LpI8zywagPQgghSULSiQ89t4tbAadW3S5hlg9qD0IIIUlC0omPaFs+rFowqD0IIYQkK0koPrQtHxV17mS7eCxeUav1QAghhJD2RvKJjygXGbMaSBpe4dSV0xNCCCEJT/KJD4dFxhqaAvj30u+w80h12L71+8ox7fVvsPNIdZj42HaoCu9+szdsXZxwtwvVByGEkOTA1sJy7QG9mI+th6oMj5vx5U5M/3gTAOBfPxiD00/oEtp36YtfocEfwIb95ejVuYPiuEl/XNhyXi8uGNY1tJ11PgghhCQrSWf50Mt2MePrncdCr2945Wscr24IvW/wBwAAu47W6Lpd1uwtU7xnnQ9CCCHJStKJDz3LhxmpXuVx+8pqNdvpiYo9x2oM2znRIk3+AA5V1Nk/kBBCCIkjySc+dAJOzUhTiZZDldqTvuhNCQRa4zw+Xl+KvcdbBYg6K8ZJzMeVLy/FmN/Mw1qVVYUQQghJZJJOfDh1u6gtHwcr6jXbiRYMvyrIdNXustBrdbaLE1Z+dxwA8PaKPRH3RQghhMSKpBMfTt0uaSlKsVBarmf5aG3X5FeKj4AgRqyWYbeCSuMQQgghCU3SiY80r0PxoTquWqcuiEJ8BAKKfXe/uRofrTsQ1g6ILNuF2oMQQkhbIunEh+Rwlle7XeqbAprtRIuGPxAuC37237XN7ZjsQgghJElJOvHhlBSV+GjQER+KNv7wNl1z0wG4m2qr5XaZv+kQPt1Q6to5CCGEELeg+ABw9sAC0zYB1QwvCgtRR4jt6hvDxUfvliJk4UXGIhEjqrE1BXDzq8tx679XorzGnTVrCCGEELdISvHxjxtGKd5bWeStUWXFEC0fKcLxYhn1mgZ/WD8d0lI0zxmR9FBZPuqaWs9b0+jOmjWEEEKIWySl+Jh0YqHifYrXfOpXZ67UK8RH62UUY0zrGsPFRxC1pSOigFOV+BDri7iR0ksIIYS4SVKKDzVedcUvDYKZK8FUXdHtIooX0e2iKT5amlrQO44RA13dTOklhBBC3IDiA0q3iR6NLZaPzLTmImX1grAQM2FE8VFrYPkIS7W1NlRLiMXNuIYMIYSQRIPiwyJNLZaOjJYKqQrLh0e0fLQe88XWI7r9uVpkTBVwKrp+ZFYgI4QQkmBQfEC7HoeaxpY2wfLsYsCpnuXjn4t3hvUTXMNFXbTMwhB0UesLsbhZJP0SQggh0YDiA+FptFoELR9B8bFhfwV2HK4CoI75MO4naKVQWz6sjEG/TyWimFJbRQghhJB4Q/EBa2ujBLNdMtJaF6Y76w8LASjdLou2HLZ0LrU7JBLvSLjlQ9hA7UEIISTBoPiANatD0O2SobEqrpU6Iepzqc8YieUjyPHqBnyx9bCiJkmstUddox+7jlTH+KyEEELaEinxHkAioDXxv3TtyRjdOx+jfvUZgHC3i1OCLhG15cNK3IkeQdfKlOe/QGlFHW46tVfrvhirj6l/+RKbD1bijR+dgnF9O8X25IQQQtoEtHwA0FiCBVOGdkXnLB8Ksn0AtN0uTmh1u2hvd9Zp8z+lFXUAgNlr9wu7Yqs+Nh+sBADMWrUvpuclhBDSdqD4gHE6arBORmMgmGobfskkG1U6Wi0fyu1Gbpdj1Q14e8Ue1Ddp1w1RHynGfDDTlhBCSKJBtwuMJ/5gPEfQ8uFLUVo+vjtabas0emvMh8rtIi5I1+TH47M24MyBBVj53TH8/YvWlN3vjyoO6zPMhSOUgo+X9mCWDSGEED0oPmCcHhsUFsEgzlRVfY4zfrcAA4uybZxL1jyn+H7mst14a8UevLVij6LNltJKzT7DUm1l0fIRHxFAiwshhBA96HaBseUj6HYJujJSU8LNHJt0RAEAvHz9SMV7PbeLKBKOVNVr9tU1L0Nz+/ur9ysyXMRF8CgCCCGEJBoUHzATH83/BrNd1JVJzUj1etAtN104V/CVqiR6yxj8AVlXMBhZMT5adyD0WqxwGi+oeQghhOhBtwuUa6GoCVo+dh2tAWBffKhrgIRiPlSzsz/QLHDOeXYRdurUyTCyYpTXNgrnCD8fIYQQkijQ8gFljIQadRn0FJviQyy9DogxH+oKpzLKaht1hYfWMSLiWjPKfq2ONDEIBGRc9fIS3PbvlfEeCiGEkChB8QHAl6J/GdTFS1O99lajTfV6FC6IUMxHy/tLT+4OoFlYmBUaM9rd6NfeGbdsF4cn3nGkGkt3HMOcDaUIcFU8Qghpl1B8AHjyosEoztcO5vSo8mjTDISKFuFul+Z/g5Ozt6X/gKxak0UDMX1VHf/RqFUpTaNdrHCeahv/NGFCCCHRheIDQJ8uWfjiwbM090kq8aEWI2akejwKK0BAZfkIumX8ARlbDupnzQBKa4JaU7y8aIf2MbZGm1gwXoUQQtonFB8mqEM87CwiB+jHfAQtEsGiZQ1NAdw8Y7lhX6IVQz0xV9U36Rxja7ju4cJ5qT0IIaR9QvFhgtrS4bVp+UjxSAoXhF/ldgnGm9TrBIyKKLNYrI4gXm4Xp7ReX1o+CCGkfULxYYLa7WJTe4Rlx7S6XYKWj+b9tQ3algvFsQaWDz3iNX+7EWtC7UEIIe0Tig8T1F4W224XkzofvtRmt0t1g/aicSKK2BGr4sNSq9giyzI+2VCKfWW1xu0ScvSEEEIiheLDhDC3S4QxH8F02oCstHxYQRnzYfUYy92bcrSqHj/8v+WYu/FgRP18sGY/fvzvlRj/9Odh+8TLzUxbQghpn1B8mKCO8bCb7ZKiynaRdWI+rOCkcqmb1oPff7oZn317CD/61woL59Vn6Y6jls7HmA9CCGmf2BIf06dPx+jRo5GdnY2CggJcfPHF2Lx5s6JNXV0dpk2bhk6dOiErKwuXXXYZDh6M7Ek5nqi1hn3xobJ8BN0uLe+DbhcrKOp8WFy+xc1lXo5WNbjUk7VrSO1BCCHtE1viY+HChZg2bRqWLl2KuXPnorGxEeeeey6qq1tLgt9777348MMP8c4772DhwoXYv38/Lr30UtcHHivC3S72jk/xSgorQOhpPgEsH6IbZ/vhKtSaxJ3YKbBmNDyr+i1eBdIIIYREF1sLy82ZM0fx/tVXX0VBQQFWrlyJ008/HeXl5fjnP/+JmTNn4qyzmot2zZgxA4MGDcLSpUtxyimnuDfyGKGeKO3GIaTqZLsExYOdherczHZZvacMP/y/5fj5+YNQnJ+JK/66BL06ZWLBA2fq9mVLfBjssxo2w5gPQghpn0QU81FeXg4AyM/PBwCsXLkSjY2NmDRpUqjNwIEDUVJSgiVLlmj2UV9fj4qKCsVfIqGe5I9W1ds63uuRVFkqzf8GN0mSZH1Sd1TnQ5tpr3+DI1UNuO/tNfhwzX4ArSv36mFHKB0sr9O1XEgGbhdxDy0fhBDSPnEsPgKBAO655x6MHz8eQ4YMAQCUlpYiLS0NeXl5iraFhYUoLS3V7Gf69OnIzc0N/RUXFzsdki2uGt18nu+P6mHYTr3Y2+FKe+JDbfkILSzXMrFKEuCzOKmLQsjqxKzXTPxcYr9fbTui25cdy8fXu47hZ/9dq7nPyO2idFFZPl3MKa9tjPcQCCGkzeJYfEybNg3r16/Hm2++GdEAHn74YZSXl4f+9uzZE1F/Vvnl1CF489ZT8KuLhxq2Uy/2dsWoyMSRrAo4lWB9UndS4VQv5kMvpfWafyzDriPV4QfAnuUDAN5esVf73AbHiKIqUet8zF67H8Of/BS//2SzeWNCCCFhOBIfd9xxB2bPno358+ejR49Wy0FRUREaGhpQVlamaH/w4EEUFRVp9uXz+ZCTk6P4iwVpKR6c0qeT6cTfJCxVv+HJySjOz3RwNrG8ejDmo/m9JElh1hHdXpwEnOo0kwzafHdM2/1id0VfPdRVY0WMFs9LFB5/fwMA4C/zt8V5JIQQ0jaxNZvIsow77rgD7733Hj7//HP07t1bsX/kyJFITU3FvHnzQts2b96M3bt3Y9y4ce6MOMaIlo8OPlvxuZr4g6mvLTOrR7Ke/fHKlzuxbm9znE2kFU5FAaB24aR6tQckio9ABD4R624X83PUNvix43CV47EQQgiJPbbEx7Rp0/Daa69h5syZyM7ORmlpKUpLS1Fb21wmOzc3F7fccgvuu+8+zJ8/HytXrsTNN9+McePGtclMFwDwR1AoY0RJHgD107zK7SLpr0irxYV/WRzWZ5aBKLISG6Ke5PXSf0ULTYPf+XUxCjhVxrWY93Xe84tw1h8W4uudxxyPhxBCSGyx9Sj/0ksvAQAmTpyo2D5jxgzcdNNNAIBnn30WHo8Hl112Gerr6zF58mS8+OKLrgw2HlhZbVaLX04djO8N6xa2PVRkLOh2gWRLfAQJTtJZvhT06pyJ9fu0s4TU8/ex6gZkpnkV1gf1JK/nBhJFSV2jH+k2CqSJGFo+bLqWvmvJ0Pnf2v0Y0zvf0XgIIYTEFlviw8pTdHp6Ol544QW88MILjgeVSFQ7EAYAcMO4XqHXCleCqs4HJGexDUerm6uNSlJ4CXgR8Ts7WlWPkb/6DLkZqcjJaP3q1R4U0TJR2+BHQ1MAXq+kEANORRlgXOfDruWDEEJI2yPyIIZ2jhOrhBGhOh8t/3okCQ+eNwDPzLGeOXGkqh6XvvhV6HiPwWwuTuArvjsOoDlNNDcjVWijnOVFATDiqU9R1xguNOoazVfh1SMaAadGfRJCCEksuLCcCVoTbyQE1DEfAH5yRl9bfczfdCj02iOFrx8jIs7foshQuF10xgjof/5IrotVmcCF5QghpH1C8RED/nTViNBrrSJjkiShMMdnub8D5XWh1x5JMlzsTs+SIB6hnuSD740yWtTF1+xgZKVwUkI+1tDGQgghkUHxEQMm9O+MLx5sXjOlvimARn9AEXAKAFOGdLXcnyg+JEmC19DtIhbtakUUAB+tO6A4JqgrGg0yfQ5W1OGt5btNF6PTwmrAaWJKj8QdFyGEtBUoPmJEYU46OmY2x1nMXrs/VL0zqBsemjLQcl8HK0TLB4zFh/BatCSIRzT6ldNp0Kqh3i5y86vL8bP/rsNv52yyNmgBo4BTPTcRIYSQ9gPFhwk56c0xuaN7dYyon7QUDyYNKgQA7DlW2/qE3zIR20lbrRDWFXHsdrGQcdJoIaNl7saDpm3UWK3zkahru9DtQgghkUHxYcJ/f3IqbhjXE3+55mTN/emp1i9hp6zmuI7y2kYh4NT+VCZm4Hg9Jm4XmLtd1AS9LUZul9Z+jPdf8/elYRVIFevKqBRGWyivTgghJDIoPkzoX5iNX04dgsKcdM39nbOsB4oG01vLahpDT/hGLgg9KuuU6b9G4gOKydzabB6yfBi4Xazy1faj+Mlr3yi2iaO9681Vin1yGwg4JYQQEhkUHw64cVzP0Gsjl4eavJaYj/LahpAocFKfQl17RKvIWPBcyhgK8bX+xG7H7WKFg5V1ivfiZ569Vhnsandtl9Y+HQ2NEEJIHKD4cMATFw0OvTa0OqjIEywf4touQaZfOhQA8OgFg3Bq3066/YjiQ5ZlzTEEBcnhynrUNzVnpOi5YPT6b4xg/RYjLGe70PBBCCHtEooPB4hP7nbcJrkt1oiy2sbWOh/C/qvHlGDdE+fih6f1Qe/OHXT7UdfY0KpwGhzjPW+txjl/XATA+sR+x8xVaPQHXHG7aOHmwnJW+iSEEJJYUHxEiB3LR0560O3SGMrkULtdslvaWO1XBuDVaCquDbf7WPPiawEb8R9HqxosWT6cWCfsLCwXtNrEiqU7jmLWqn0xPSchhCQbXNslQuzEfGT5mi93dX1TKEtG73A7/WpZPrTiQAI6Bce0kCT33C7qkRjHx7aO7O43V+G7YzX48mdnoVtehvE5XDJ8XPXyUgDAwK7ZGFiU406nhBBCFNDyESFaFoqbx/fSbJvVUjOkpsEfSmfVmzOtig8J2mu7aC42Z3O5+gYL4kMcplWxYnVhuV1HayDLwGtLv7PUr5vsL6uN+TkJISRZoPiIEC3x8YsLB2u0bLV8AK1BnXoTsdfiN6NXXl29rckfUAacmmgPCUCTjZiPt1fsweBffKK7v9EfQEVdo+5+q+OKFYwhIYSQ6EHxoUGRTk0PLexMUb4UT0gUBMWHngtC03Khg5aVRO12qW3028ok8cuyrZiPB/+zFg0GqbnnPrsIw574FEer6i1VV1WcQ3j97NwtmPqXxahpaAprRwghpG1A8SHwwR3jceaALvj3LWMsH2OnTockSeiQ1lxGPZixoveEbdntorO2i1q81Db6bQWc+gPWxIdVdh6pBgAs2XE07LPVNfpx84yv8a8lu0xjUZ6ftxVr9pbjreV7FNtdt1PQ8EEIIVGD4kNgWI88zLh5DPoXZls+xm6F0mA2SxA9jaEVMKqFJFmzfNQ1BOC3EXAaCAANFtwukhRe9MyIFE+43Hrj692Yv/kwHn9/g6Yo0tJJ0apBEsTo6kcS3LrtUBU2lVY474AQQtoBFB8xpoPP2gJydkSNlYDT2kY//MKEbRZw+unGUjRZdLsMf/JTiyPVFkrlwkJ5H645ELY/0XAal+IPyJj0x4U477kvUGkh/oUQQtorFB8RYiclFgA6+JTZzXrH+y3OcBIkpGhEp6o31Tb60ST4XcxWjP3V/761bF1QFz0z2u/1SGFWF3Fxuf9+s9fSOdsiTcJCfUerGuI4EkIIiS+s8xEhaSn29FuHNOUl19MuTRbXk5ckoCQ/M2y72u1S09BkKhLUWHG7WOunddL1eKQwy4GZ0JIhY+mOo/i/r3bptnF7bRejWB6uI0MIIZFBy0eEZKal4BcXnmi5fXqq0u2iKz4sTvwSgAFFWeHbVR3vOFytjPmwYFnx26zzoYeYBeOVJEXKLwCYnkZuLv718fpS85O5RDT0hSLbKAr9E0JIW4HiI2Jk3Dy+N167ZSwAINtnbEzKTFOJD51pzkq8BdAsMkryw9eBUWfAfL7pEPx+UXyY921F/1jpRzS4SFL4Mf6A8WfVOkU06oFYEWSEEEIih24Xl5jQvzNeu2UsenYKd4GIZKgsH3qBpZbdLgB8qeEaUt3vseoGRZ9Weo/GZOwPyGH9Ok1ccXt8Nr1StlHWWaHQIYQkLxQfEdM6y0/o39m0dYba8hGh2wUA0jQCTtUWlYAsKzJcrJRXtxIjYjeOpFl8KLeZjUVrov580yG8KdT6sFNvRQ9xHNGI61C7mwghJFmh+IgxavGhF13QaOKKEA9P1arFrurWH5CVlg8L86AV64tVC404DvURdgUMACzbecxW+9LyOnTOStPMDAqiEB9RiPpQFHlzvXdCCGk7MOYjxqjdLm4EnGpWONUQH36F+LAgLCyMwSxeI7y9HGbpsJpW7JTlu47hlOnzcM3flxm2i7YnRJbtiT9CCGmvUHxEiF3zfHjMh07AqcVJXc/doOV2abIZcGplDHYtH01abheTPiKdqF9vWRX3613G1pKoi4/odk8IIW0Gio8IsTthpYdlu2hjx/KhuV3D8mE35qPRwhjMhENYe9kdt4ua4Md1Us8kSNRjPqJbEZ4QQtoMFB8xJlNl+VDX/Qhip8iYle0BWWnJsNK7FZeKbcuHXw5TbOZFxqxxrLoBJz7+CS598UtHxytjPoxwpkyUsot2EEJI8kLxESG23S4qy0d4AGozVkub6wVGqreHx3yY923F8uEk20V9iJn1ZON+awuxfb7pEABgzd5yLNpyGDOX7bY1NusfxZlwUKbaOuqCEELaBcx2iTHpqpoc6qJjQU4qzsMXW4+Y9hcUPx4pvJiXiD8go77J+sJygLWYD7vBon5ZDks5NbOeLNlx1Lxj1ee94ZWvAQCDu+VYH1yUBYGVa04IIckAxUeMEdNiU72SdposgGln9kMHXwr+8cVOHKmqN+03xetRlDFXE5Bl1Db4Q++tTIOvW7Ac2J1PNQNOXZiUJR0b0O5jNZb7UIwjKnU+tF8TQkiyQbdLjBELgqkzX0TSU7247Yy+mHffGbhgWFfTflNUubXqLBh/QEaNKD4sTPjReFD3+wNhE6+dgmp6yJA1XWANTQHLn8PNOh8rdh0Lc/uI46AVhBCSzNDyESF2p6hUYRXcDibrwABAbmYqLh3RHf9be0D7/C0zbpj4ULULs3zEae5r0qjzYTdoVRNZO/6mQUPs6KEsAmZ0lPm3fvlflwAAenbKxPh+ncP6tFkehRBC2hW0fESI3WlTYfnQifdQYxTUGZwG1ZU7tWI+ahtbxUe8nrwDshx20Y5VN0Tc798W7cCuI+EuFiNXlBqFNcjw8li/djsOVwn9iz3Q8kEISV4oPmJMmmD50As2VWMkFIIiQ13lVP1s3ux2aRL6tHRq12nSKK9+qKLOlb6fn7c1bFuz28XahxVbObk+hyrr8MrinSivbdTs061sl4amAD5adwBHLcQCEUJIIkK3S4T07ZJlq70YYJqZau3yG2XdBsVHqknMR0CGwu0SL/z+8FVtS10SH1o02FgyVxR5Ttwu1/59GbYeqsLK3cdD2xTpzaLbJQL18eKCbXjus63o2SkTCx8403E/hBASL2j5cMh/bhuHWyb0xl1n97N1XKq3deKy7HaxMFF5vcoJUWttl5rG+IuPYzUN+HTjQcW2aFph6psCKuuDjL98vhUfrQuPoQlEaJnYeqjZxbJw82HNPvVe2+XjdaUAgO+OWs/kIYSQRILiwyGjeuXjse+diMw0e8YjR24Xw5iPYMCp8qs8UK60JvhVAafxYsaXu2I6aapjPlZ+dxy//3QLbn/9G8z79mDICvPd0Wq8+XVrdkokekgUlQFFYTd3LB/x5P3V+3DxC1/iQHmtreNkWcYnG0qxx0bqMyGk/ULxEWPEgFNfirXLb5QNEvSuqLNd1OKjyR9QFBlLFtTi42BFa5zELf+3AvO+ba6KesbvFuDPn28L7YtEHIiiUuHKUVhW2qb4uPvN1Vi9pwxPfrDR1nGfbjyIH/97JU57Zn6URkYIaUtQfMQYMebD67F2+Y0tH81kpSstMGq3S7wCTONNg19p7VG7sFZ8dxyaRHC9xPotetc9Eu0RjUXv7FJZ32jeSODrncYrChNCkguKjxgjul1SvdZmEcOYj5aZ6NcXD1VtToAZKgGobwwohIRayOlZnyJJhdWzfChfO+6+TdJGDT2EkChB8RFjRPdIikXxYeh2afn3xG45uGVC79B2L8UHgPBsF3XNFF+qjviIxPIhiA9Zx+3SVmM+gtgdPuuaEEJEKD5ijGiRUAeJ6jGypKPuviHdWxdOE+WG2u2SrDQ0BRQTn9qK5EvRDvqNxDIhBiGL2mfxttaFAtu6+CCEkEig+Igj6iBRPU7sloOZPxyr2HbWwALcPrEvHpoyKBpDazeoF7JTu10OlNVqxtREEhCq53Z5dNZ6oX/H3ScEti0fbfzzEkLcheIjjqhLohtxUkme4v2Q7rl48LyByNJZH8buUvftFbWwUF+XfyzeiT4//yjsuBXfHUeTjQJlYlaNGHCqJ2Ki/fVEO5uGbhRCSCRQfMQRq5YPIHyVVa1jxenAxrzZrvG31JcIYpQ5JPLyoh343SebTdt9tvEgth6sVMSWiEHFeiIwIDeXu39h/jZsF9Z/sYJZMPHhynpM+O18PDt3i61+CSGJR3mNvcyytgLFRxyxGnAKhKdXqtdyAdpXQKNbfLX9qCJ+w84Kun9btMO0zQ//tQLnPLtIEcjqEb4svdMFZBm/+ehb/O6TzZj6ly8tj8kKLy3Yjn1ltWFr3RyurMf6feWunssqbbWuCSHx5MM1+zH8l5/i6Y83xXsormNbfCxatAgXXnghunXrBkmSMGvWLMX+m266CZIkKf7OO+88t8bbrhhYlGPeqAW1+DCzmui5DKYMKbJ8TjuM79cpKv1GirrImLr4mhO0DA+iRUXcHxSB6slXloHPWwqcVdU3wQ5mklVPeI7+9Wf43p8XY8P+yAUItQQh0efJDzcAAP66cHucR+I+tsVHdXU1hg8fjhdeeEG3zXnnnYcDBw6E/t54442IBtneeO/2U/HkRYMxeXCh5WPUbhcty4eI3uRgtp7M2N75lsdk5XyJxssWrBlmaH1WPfdKcHOjX7k/IMsoq42POXV5HAp+tZGfByEkRthe1XbKlCmYMmWKYRufz4eioug8YbcHRpR0xAiD9Fkt1FpDS3zcdGovvPLlTlw0vBs++/Zg2H7AfD2Zk3t2xN7jtdhX1rp2x++vGI7LR/ZAr4f+p3tcspcVUaxeq5Fd06iyRAVkoMbhWjtm19pj0iAeQqCtiFNCSGyISszHggULUFBQgAEDBuAnP/kJjh49qtu2vr4eFRUVij8SjjrIUMvtUtIpE5ueOg/PX3VSWDGtIGYL4UkIN9tbCYxNVWXunNa/s+kxbRWtuV3vegctIjuPVCu2O4mB8AdkSwuzxUII2h09s2MIsU97Fu2ui4/zzjsP//rXvzBv3jz89re/xcKFCzFlyhT4/dpPedOnT0dubm7or7i42O0htQvU84neujDpqV5IkqTr909PNV9JV32op0V8TDuzr+4xavFh5hZqK7wwfxuu/cdS1DUaWylE8aG1mNz3/rxY0d5JEbN73lqN056Zjw37jQV6TC4963wQQiLAttvFjKuuuir0eujQoRg2bBj69u2LBQsW4Oyzzw5r//DDD+O+++4Lva+oqKAA0cBuwKnek3iGFfGhmlmCp3pg8kA0NAXw9y92hh2TphIf7UN6IJRu+86KPZg4oADF+Zma7UTBoSdERNSWj7V7yzCsR57hWD5cs9/KkLmuDyEk4Yl6qm2fPn3QuXNnbNu2TXO/z+dDTk6O4o+Eo55QzCwLek/WGTprmbSeJ/xYcZ0Yj85503QWaGsvPPb+Bpz2zHz8e+l3mvubdASHPyBrCkH1pqkvuJdum4jag4aPtseS7UfxgUXBS4hdXLd8qNm7dy+OHj2Krl27RvtUSYWdGiEivlQvJMnYDK7eJwofMZjxtP6d8cXW5vVK1Cv0aj19ezSETVvjmTmbNF1XYqptQIgtDcjA8ZqG8PYaqbduoc6MUuPGuezGcNDt0va4+u9LAQAnds1Gv4LsOI+GtDdsP65WVVVh9erVWL16NQBg586dWL16NXbv3o2qqio88MADWLp0KXbt2oV58+Zh6tSp6NevHyZPnuz22JMap6b1FI8UFp+hRu0SEK0s4lkfmjIw9Frdp9bozLIw2gQ6k6iYautXxHzIOFJVH9Y+mkXgzC4zC9ARO+wvi7w2DiFqbIuPFStWYMSIERgxYgQA4L777sOIESPw+OOPw+v1Yu3atbjoootwwgkn4JZbbsHIkSPxxRdfwOfzuT74ZMbpNJ7ilcLiM9SopybR0yIKCFFwmAkaIDHdAXbRm7ZF14o65uNYVbjlo8ygZPKyHUfx+SbtVGkrmAWc/up/36K+yVmabxD7+oWChxDSim23y8SJEw3TBD/55JOIBkSs4dSK4PV4FC6SId1zsH6fMntCqxpn63nFvlrfqGM+tIbXbK1p25NQVX2TZqE2patFFB9Ao4av6RcfbNA9x5UvN5u7l/38bBTmpNseo5nbBQDW7i3H6F7OCsoB7nyLZTUNuPXfK3H5yT3w/dEMMk9U2vb/WJKotO8owXaMUytCqsrtMv2SYfjqobNa+4UUFpfRpCgd3npiMeNGHfOhZZtRbzGzwCQqhyvD3ShNgvpQWD4CsmU3h7oUvNZ5rGDltxFrz4vW+Z6ftxVf7zyGB/+7NraDIbbgujwkGrTNuz9xXMvB65EUVgpJArrlZSjaqG824sQqWly8Hm0XjB7qSdHXjjJkRIFRUdek2G715n3DK8sUbZ3e8638NGI9oWidrqLW3po2bjB77X48NXuj5dWNCSHRof3c/ZMMOwGnL18/MvRaHfOhlTWjvi37dRZNS/V6cPWYEnTMTMV1p/Q0feJWT0A+k7TftoRYPX3RlsOt22WlS8aIpTuOKaxOTgNDrfw2Ip167YqXRKlwesfMVfjn4p34eH1pvIdCSFLTfu7+SYYdw8fw4rzQ6+aYD/1AUa00XHFRNPG8HknC9EuHYvkjk9A5y6ewiqgrgs64aXTYZJrli3qmd8zQK+oWkK27XYLtgzidrhMxsDfRLPdaGUhEmwT76kg7geKjjWIn4FR0j6R4JKSmCO4SjTLt6snSLzy6izftvMzU5j5bBIzoCqpViY8zBxaExZJ0cEl8ZPlS8IcrhrvSl1P0BEZDU8BWbRNlaXbt12aIAad6Rc4STQyQBIa/FRIFKD7aKHaebsXAUElSWju03C71qsBH0fKxW1jYLNxq0tqXuGLrqsfOARA+QUciPrLTW4995vJhuGxkD8d9ucGzc7dobi+vbbQlHBQr4gqv7QSfir+N85//Auf8cWGYAInUDWJ/Ybn48dKC7XhhvnaFZUJIfKD4aKPYER/qUuyitUMrUFQ9UYmZLEYFhxSWj4bWYMKOHdIAhD9tZ0cgPl645uTQ60TImlnx3XHN7WU1DbYsH3rum0dmrdfc/vmmg2E1O8TvYfPBSuw4Uo2DFarvLUI1YNdyEi9LS0VdI347ZxN+98lmHFJfA5d44+vd+GRD+40hSZR4HdK+iP9dmzjCLKiwV6fmBdBGlOQhReVaEd+Gp8gqGdM7H1NP6h56//MLBgEA7pnUP3xMgrlf7XbRIkuwXthdG0a02KQmcNbM8ZpGWzEffkXb1tdzN2oXHfvBqyvw9MebFNu0fhvhlo/YEq8JrLS8VXDsPFIdeu1Wts+uI9V4+N11+PG/V7rSHyHJQuLetYkhZjEfr/1wLKad2Rd/u25kmOVDFAlGZdG9Hglv/3icYi2TM07ogvVPTsY9k04IO6c4JNHtoofodkm3KSDEz58Ilg89jlc3KFKVzWgSXFyiXhjeI1f3mJnLdpv3qxYfkVo+Ijs8ZhzQER9ucUxj3R5CiDntJ90gyTDzuvTomIkHJjevvSLWNFBXvzRaoC4vI1Vzu16WiigIai2ID9HtkpHmVdTHMEMhPhLY8tEUkHHvW2ustxdydkWB0DlLf3kCtWVFS5e6HfPRVjgoiA9RiLhFAiYWuQ6Dk0k0SNy7NjHETraLR235EGt1aGS7BOmRn2lrTGK/6idtLTLTBMuHxkqxRogfqT0VKxNLsYuuASNLkvpaa/02Gv1K60vMJxSN88VCAB0WsrOiUVhNdHG110qg7fRjkTjTfu7aScbgbjmOjxXnJrUwgSThxWtPxtDuuXjuypNs9StOevef0+yW+eGE3rrtM9LsLUynHGbbsHzYRbR8iJqixiCGRpaBfWW1+GZ3c9CrlixVl26PeD6xXWQsPui5sdwaj3itWTSVEOvQ7dIGuXl8r1AGiV2yfCmmC4+dP7Qrzh/a1Xbfoo6546x+mDK0K/p07qDbvlMHH24c1xOSJGHt3jLL57llQm/FuRI55sMuooUi6CrZe7wGa/aUGR43/unPAQCf3nu65v6GMMtHjFNt4/T4LFpXFK8tDGfB5kMAgIkDCnTbiELeH5DD4qvaKrILxe4IMaL93LWTCCP/vx6Pf+9E/GB8bwzpnhO1CpiiNUKSJPQryAq3rCjaA09OHYInLhpsq1z83ZP6K6wsiZztYpeGptZbfTDz5aqWVW6tsGZPGeoawwNc1ZaPWGN3Aquub8LstftRVR/Z+i96dVPMqGlowk0zluOmGctR06A/BlHIOy2H7xRZlhWWMnf7jkq3hISg5SNJ+IGB+0MkEl2SYTtuQxJeWz8u1eNRCKj2ZPkQM2MCIctHreXjX1ywXTOrQy0+Fm45jLeW78GvLh6CTg7EbLQnp/vfXoM5G0oxeXAh/nb9KMf9iMO0Iw7EGJu6xgAydQyN4u8w1uLjzjdWYeHmw/jiZ2ciT2+ADqH2INGm/dy1k4hEXLsDAE7u2dFWe/FzmLmCRLweSZG90Z5iPsRqslaCdtXopZOqq9bO+HIXPl5fil//71vb53CC3Xl5TkvRrk82aNc3cXJiN7XBXxdux0V/WYxKxQrG7vVvhdlrD6CyvgmzVu2L6nnaayAtiS/t565N4s69k/pDkoDJgwvtH2yzXLwoPtpTtsvirUdCr/WqnTpBHfMRpNSg6mcgIGN/mXWrixHxmr4Ulg8Xr+fTH2/C2r3leOXLnaFtbn5fdrDjsrQKYz5ItKHbpQ1ix0qgebzBzSqS+1ifLllY/fi56JBmzf2iiBExaHfXWf3g8Uh47rOtAJozdEQLQXtyuzz7WesaMQFZdu2pUy/mwyhl+6fvrMG7q/bhpWtPxhRVALLdNNlYPj3XN/lx3T+WYUzvfFVMhjAekz7E4Rr9NkWLUrwsBNGwhFJwkGjTfu7axDJa96oxvfMBAJeM6K6x1zq5GamhVW7NEOM81DfQ749qXSjuvnMHoEdHZc0RMTbCKKi1LdMUkBUWi5vH93Lcl3r9lyBGE9e7Leb85+dtDduXyJb4j9YdwPJdx/HC/O262S5mWG0rCo54WT6igfj9JvJ33d5pz5eelg8CAHjzR6egqqEJOenaVU2jgVdh+VDOguoncnUwn1i/ob0SCMgKi0VRTrrjvh55T3thOisme3WBMtew+BU2+gO26sA0NGlXibU1iSqyZPQPdJpN4ybRkN7JUgGXxA9aPtogkZpZtY73eKSYCg8AGCkEqKrHpJ4U1SbtAUXZEZ/faL2URKCu0a+YSKMR22LFaKQVL6Kej/ccq8HUF77ER+sOaPYRyVQ2bvo8PP7++lARNTs4zXZRHqffTrR2xDrbJYTODeFoVT0WbTnsaqxLe2T+5kO4/fWVOF7NdXpiCcVHG6RrrvMnYCAx1qO46+z+KBCe5NX3T/WkqJ7/uuVl4NN7T8eyn58dpRHGn4feXYdr/r4MQHOQrTcKsS1WyvSb1QhpaArgoXfXYs2eMtz++jfajSKY/45UNeBfS77DpS9+Zam9aEWTdSwYZvEZoqgwsgKIqxCbiY8vtx3BA++sQXlto2E7u+h9g+c8uwg3vPJ1yH1mB+VHad/i5eYZy/HRulJM/zg2mV+kGYqPNsQ/bhiF287oiwuHdYuon2hEx9vlvnOUq+Kq3S7qIfo1buwnFGajMAJXhBkpCRBLsvlgJYDmdGJvFL43vR6f+GBD6LWR+NiwvxwnPPoxvtx21PA8Zmb8jfsrbMdMPDprHa7/5zLDJ3vxvOpmlXWNWLHrmKYQCVhM0bUT83HtP5bhnZV78ftPNhu2c4tjLU/y876NLF05WWI+orHwINGH4qMNMenEQjw0ZWDEAZan9e8MILGyRMItH6qYj2iYjk0mc7vrzUSTVK/HViE2q2gJ0er6Jrz61a7Q+0ad+JqahiZc8KfFrozj/D99gadmb7R1zGtLd+OLrUewao+BO8Yg5uOyl77C5X9dgndW7g0/zGLMh8LtYjE0Zp9L6ctBzDSpE82aLIJDJG5usyQlce6uJGZcf0pP/PH7w/H5T8+I91B0UYuPswY2r6/Ro2NGzMaQ4rV3187NiF7MTFqKJypZPVpdqq1MVfVNePcb5QQtAzhcWQ+rWLmvi4LHDkbBx+IetYVjy8EqAMB734S7JQKyvsVExG9RpIi4/S2apd47sXQqs4SSA6vikbgDs12SkBSvB5ee3MO8YQxR3yDV98vi/Ewsf2QScjJi95O1axmKZrGzNK/HUnyGXSrrmvCneVtx8UndUdIpE1sPVmLpzmNh7e57ew265bUKP1mWbdWbUWadyDFz/ck6MRnzvj1keJwi5sNAVIgWuaBoq23wI8NirRu7aF07U8tHVEbS/mCGT2yh5YMkBOIN8vmrTtLMvOmS7YMvJTo3dS3sul2iuaJpWooH0fACLdlxFH+cuwWXvvQlgOYgxcdmaaflPviftYr3WnE4emitKOvWrT6sH+FrEIcoBi0v2dEao6KedA5V1GHFd62uHKOPqQ5ifX/1Pgx6fA5mCJVPw4bn8GeyfNcxjPnNvLCMIrPuHFk+krDOB5OCYgstHyQhEO+PU0/qjsq6RizbeRQXRBhcGwl23S5RFR9RsnwEOVJlnma4+1iN4r2dAFHFZKbTJhoxSAGVxcUKY34zT9WHtZgPfwC4+83VAIAnP9yIm8frLebo7Hv8wYzlqKxvwu2vf4NdT1/Q2lsULB8Kd1WSWAS4hk1soeWDJATqG2R2eirevHUcrj+lp+U+/u8HYzCmV35EgZk3ndor9Nqu5SOa2TGpKVJCZCmJOK3oqXeT7+BzZtWyapmwY6lR9qG/z0mdD6dfo976POYxH/bPlYwTMS0fsYXigyQEbkysZ5zQBW/fNg59umRZO6fqfVqKR3HTTU0wy0c0Um0jwbH40NnewWfdEKtc+Ex/HGJ1Vr3xms2zhjEfDsqrO/0Wnc6NTixmyTgPM9slttDtQhICN6dVp32leT2Kp58Uj13LRxQDTlOik2rrFFl2XjFU77AOaXbEh7V2SreIjvho+fep2RvRJdsXtt9IU4jjeOjdtfoNBVzXkFEOOE2WOZmWj9hC8UESAjddCk67SvFKigk11Wb2SjQtH6ne6KTaOkWGjCbHMR/ax6XbyBAJWAkiARRj1LVMyMDWg5X452LtIFEjy4foylm/r0J/IAKRrkqtHpNpb6zzYYlkdDXFE7pdSELg5tOg05t7qtejqObZQWMy/PHpfTCmVz46Z6WF7cvvEL5taPdcR2NR40vxRjXgFLBfyE3fjaG13bxiqB1tpe7CH5AxZ/0BHKqoU3z7VmMyahu1V/1tPk5/HE5cT258jeJHMRPu6t9NZV0jdh+t0WkdPIHmy3YN3S6xheKDJASuul0sdqZuN6Z3viKoL03D8nHz+N54+7ZxmqnAV44uDtv24Z0TrA3GhCyfNyqptiJ6AY1aGLld1u0rd3R+O78BRYorgJnLvsNtr32Dyc8tUkyWlmI+YFyzxCimJJqLtn21/QjO+sMCLNl+NDiQ1vPasHyo95/ym3k4/XfzsfNIte4xytTo5JiUWWQstlB8kIQgEWIpf3PxUIXlQyvAMxTWodp1xgldolpkLCMtJerZLo02xMfWQ1W44q9LNPdd949lYdvcrhuh7uPTjc3rlxyvaVRM0oqYD4PzGl1af0BGnY5lxEkGjdWv8Zq/L8OOw9W4+u9Lw/YFFJYPe+erbmj+LF9uO6J7TLT0RnV9E9btLU9IQWNm+Qha1w5WcA0YN6D4IAnB+H6dXevL6SSdm5mKekF8aPVj5PqIplukQ1r03S5mq9fqMaAwG89deVLofUVdU1gb8bYevMmrJyCn35vaCiMKAjHmQ89KYTYP3v/2Ggx8bA52HK4K2+fE8OFGzIfC8mEacKrdwOi4aEmDS1/8Chf+ZTH+pyqUlgiY/Q7e+Ho3bnvtG0z6w8LYDKidQ/FBEoJrxpTguStPwhcPnhlxX+I99cHzBuAPVwwPvX9oykDDYxWWD43/HXoCQJKcWW+sxjlkpnmjnmprx+0i4vVIuHhEd8W2Z+duwfinPw89JcoqN0mkKN0usm5Wi5VsF8D4u9tU2ryy8Iwvd4WPI04pEuJE+cXWI7qWGUD/s0Uigo5W1RueU4/gKs2zVu13fO5oYWb5+KxldeDK+nBxTexD8UESghSvBxeP6I7i/MyI+xJvtrdP7IfLRrauYyNO9lq3XqX4CG+hJwAkOBMfVjNkMn0pUU+1rXZ4U1V/hh4dM/D8vK3YV1aL15Z+B8B4gTcnqLsQ/fUBHcuHkfiwYlXSKjrnqHCZhe9x1qrwxe5ExM/47jf78NN31uifzkAw66EQi6qPeLCiDiN/9Rkm/Ha+4RiNaXtuF6eWQaINxQdpdxjdVM2e9uqbWp/menfuEH68zv8YSXJWgdSq+OiQ5o16qm15rTPxERzXScV5AIBhPVozfLI0CofJ0F4R13HAqawUAaLIaBKsOXqTiwxrwlErANlRtkvLvwcr6lBe06jZ5p63Vhv2of4ss9fquzH0LR/6GH2qYADskSrrqxq3Bcx0pJ2YKGIO63yQdkck5mQx5mPamf1QUduEPl064MkPNwLQt3w0n9c+zYXJzG9qGWkpUY/5qKjTngjNCJaVv2JUD6zeU4bS8taAvE5ZzUW7xBv7Xz7fhpcX7XA+UGin2gYJfleAyvKhJz4sWi+C4sPIKmAFSZJQVtOAsS1ryIjrtBghZqDY0TyiZpUtxooY1WVxstaLLMuOrlVdox/+gKxZ/XbuxoP4eN0B/OqSIci0UaBOD1o+YgvFB2l3GFo+RLeLRkMx7iEzLQVPXTwEW1v81IBBzIdOf2ZYNWY0B5za7t4WFbXOxEdQkKW2pAJ9J9SQ0JrY9YSHncsnC/OADH0LhNUKp1bSLNNayu2L3Ti1fGw9FB68agc7ritRjFsdr0LoqK6NExFxwytfa1q7DMcgyzhl+jyU1TTi21+ehwxV3Z0f/WsFAKB7xwzcf+4A+4NSYXZp6ik+XIVuF9LuiGSO1nq6Ed0dRhXUnZzXqtslFhVOtbJUrBC8JsFVgI9Wt66Q29iS3+q2h1/99K03qVqN+bBSYCpo+Yh0sTpJiryujdM4V/F6WLUQuvHdfbH1SChw1yqyDJS1uKW2GYi1fWW1EY0tiKnlg24XV6H4IO2PCNwTWk83qRbWbJEkZ6m2XovrwZxUkheVJedFHpu13tFxwTVttIRUU8tjs9t1HZR1Q2TdicMfsLawnJXhBQNOxXM5yXZRByc7sZ7YqcYpfi2KkvgWgz7U57L7VTr97sWjKuv1rXJNRgVc7JyPMR8xhW4XQgTqNdIHi/MzMGlQATr4UuBL0Vt/RHKkeVIsWDOKctLROcuHo1UNpm3d5q6z+mHjgQp89u0h3TZBi4xWNogdP7mdOUpd4VTX8iFMTEYTtpXJPCQ+hI/kzPIhQZz5mwIBeD3m69rIBoLA/Hwt5xImUMsBp7LBPhM+33QQfTpbW2U6bAzCZ6wysMo5XV1ZDWM+YgvFB2l3GN1UzeIytO4/kiThHzeOVjVUt4me28UbmtxjXwb2vnMH4M43Vhm2CQ5LS0g12ngqNbv5p6e2ihv15KgnAsQn/S0HtU33MqyJiDSvB7NW7cPP31vXeqzDeU/8GTb5ZYjxlFYsBXbOK57L6vdhZSFAM77Yehg/eHWFaf96iJqiukFffDS5VBfd7Pdn57fsFolYCdYt6HYh7Q4jfVGYE75kusjLN4xCbkYqnr/qJMN26hVdHQecWvgfGOxWy7LglEtP7m7eqIUmE3Nz0HWkNb6gqbq2wbwgld69fVDXHABQZDSoi4zpuT8sB1hauclLzSmwNRY+i0k3Ched+rekN2bx52VLfKisLEGMJlujzBqrE+LK745bHKH5GKrq9a+5e5YP4/12LB/LdhzFRwlYxTWRoOWDtDu0JMBfrzsZ3+wuw/lDugJYpdtuXN9OWP34OaZCQuuG58ztYi4ogpYPrToTTjn3xCK8+41xIasgZr5uryrgVH3s/rJarLAwEelNaqmhLBPF43iIQCB8Ag9i9anYyvzl2lOoykqmFnd6n8W520U8V+txeucxOi9g3e0S6eUSjzdyu1j9HObnc8/tcuXLzevxLHxgInp2Cq8XRGj5IO0QLeFw3pCu+Pn5g8IyRq4ZWwIAuGFcT8Pj1agnNS23y8d3n2baj5UElmATNy0fVrNsAHNzc7AvrT4b/AHc/aax2yZIUNAt23FUsT3ozhGtGwHVRKxr+bBgKq9taMJby/eYtnOrkroESZkxY9HyoRyLnVTbVqys8gtYd7vEqry8UfXdmFk+HAScijVviBJaPki7w44B4smLBuOyk3tguFCV0wrq+5AEZYXT287oG3IXqHn0gkH41f++BWDN8hE00bsZ82FHx5hZD4zcLk1+Gct3WTO/B2/+wafGICneYHGv1m1qt4BezEajhYlpy8Eq3XgQ5fjcmeQkSb8EvNZ7LcyGoldMTOzbKEtE3GM0HL8sw6PzPy7SqyVeoyoD8eFWtotb3y+xhu1HqUWLFuHCCy9Et27dIEkSZs2apdgvyzIef/xxdO3aFRkZGZg0aRK2bt3q1ngJMeW0/l0AABmp5hkEqV4PRvbsGJrgrOLXsHyID/5a5z5zQBdseuo8/PC0PqFtVmp3GGWTOMVqii9gwfKhEXAaDA5t9AfQR6NMvRZ6T9FB0SUKDHWqrd5DqVtPxYB7lg9AKV7Vk6cblg9xtxhfYt3yobjAqp3Wx6FHaUWdqdVE7NporK5ZPqLgvnG6UnMyYPtuVl1djeHDh+OFF17Q3P/MM8/gT3/6E/76179i2bJl6NChAyZPnoy6OpqfSGy4bWIf/PayoZh73+mG7SK5L2g9nYr9ZaSF/9eSAaSrRImVVNtgEzfrfNhZIdcs5kNLHOVlpIWO7VtgLdVSbyILWocU9TUUr/WPNQuWtYNbMR8SVOvPBNQxH8r3we/fqjWieb/QwEHMh054Tct74dobXV6D67VhfwXueOMbg4OtW07cynZxy/ChKKVC7aGLbbfLlClTMGXKFM19sizjueeew6OPPoqpU6cCAP71r3+hsLAQs2bNwlVXXRXZaAmxgC/FiytHl0T1HOqnrcKcdIh3eStWF8Ci5aPlDuZmhVMxbdUMM/ERFFBiwGleZipKK+rQ0CRbfjLVc52kapQ1F5v+c/EOHKvWroHipuUjkr7UbhAjt4v6PFpPz2YWB/Fa6mW7vLxoO3IyUnDt2J4wwsgi4GhV3xY+WldquF+xho6BFIlVnQ+riOOh9tDH1YDTnTt3orS0FJMmTQpty83NxdixY7FkyRLNY+rr61FRUaH4IyTREW8wFw7vhnvPOUHhdvGZiI8xvfIBANeNNRdJ0TDdigt19RcsE91y08Oe1sx86sFAUzF+JTcjFUCzcLFaGVLv3h/sX29Bt292l+n2aSXmwyqRdKUsLCqpVt5VxXxYiGEws8Io3S6tr0UX2vGaRjzy3nrTyTvM8uGC28UKemJTjVvZLm79VAIqoUm0cTXgtLS0WckWFhYqthcWFob2qZk+fTqefPJJN4dBiCUiWf1WvMH8+eoRzf0JdxqfRlqseAN9/Udjcaiy3tIIorGki7jUfVFuumKhMwnKCccsyj9omRFdSHmZzeLjgzX7MapnR0tj0pvIgu4ccZK0WvjKTctHJG4X9YRktOCd+r3W96/1sWRZRnWDH1m+FN0JUEvYBGQZXtUvUSvkQ5ZlSJKkdP9EM9vFwPUj4lp5dZdWIFJYPqg+dIl7qu3DDz+M8vLy0N+ePeYpb4TEG82YD+G1lvgQSfV60D0vQ/PJqCBbWQjNyZoxZoiWD1GISJKEq8c0W2Mm9OsMALjvnBMM+9Jyu+Skp4ZeW6nxATTftLUms9Y1VVq3WdUB7gacOu9LOSEp3zcGApBlGXuO1UCWZY0CdtbcLrf+eyWG/OIT7DhcpTL9CwGnGvERWn0ps4lkPP/ZVkz47XwcqqizHAgaKVbFgHsVTl3pRuXyInq4Kj6KiooAAAcPHlRsP3jwYGifGp/Ph5ycHMUfIYmOdhn21tdaBcG07m1awmJ8y6Tf2sbu6Mzp4Gt1C6nH+tj3TsRfrzsZL113MgDge8O6YenDZ+N+HRGiFXBqVA5bD1nWFnVi/ZCg9SEeaZHPf+Y8a085XEkxQfkDMl75chdOe2Y+np6zKXxC17J8aMy3czc233dfX7ZbN+hRy0qgdSnV2579bAv2ldXirwt3KK796j1lOP2Z+ZizPtyyHek3ZFVsuiWA3AooDtDyYQlXxUfv3r1RVFSEefPmhbZVVFRg2bJlGDdunJunIiThEIVEmtdawKnWrUmdARONG5i4QJ46hTc91YvzhnRFtmC9KMpN1w149Wq4Xeyk8gbxB2TNp1ixvok/IGPrwUqU1eqvchotqiMoq652g4gT1NGqBjw1eyMA4G8Ld+hnuwh9PPfZFt1zybJqAhT2aWX/aFs+lP2JiJP9Lf+3AruP1eC211bqjscpSjGgLwwSLeajSefaEyW2Yz6qqqqwbdu20PudO3di9erVyM/PR0lJCe655x786le/Qv/+/dG7d2889thj6NatGy6++GI3x01I5ETxzuCzkU2iRl1zJFLLx5kDumD+5sO6+63WD9HTQN4WcSCKrx+f3gcfrtlvfZBongS1JhIxkHXjgQpc9JcvbfWbCKizQsT36ok7LNtF44f66caDYduCyJBVqbai28Wa5UNE7Cs1RbJsaYi4vLrFvhIt20UUfrG3zxnz2tLvIEkwzXCKBbbvkCtWrMCIESMwYkRzkN19992HESNG4PHHHwcAPPjgg7jzzjtx6623YvTo0aiqqsKcOXOQnp7u7sgJiZBTeue72p/C7aIxoZvV6eielwEAuEy16JuWa0ZMle2aa/x/68Lh3dA5S39BPauVU/UCdIOWD7GGSd8uWThvsLarVY+ALGu6BcRYkvmb9EVUIiMLBgd1nQ81YTEfDsSnXveWLR+y9gSa5vXEzOVlNavGrdVm3fpYorBMpKqp5TWNeHTWejzy3nrDirGxwrblY+LEiYa+MUmS8Mtf/hK//OUvIxoYIdFi4QMTsWDzYVw5utjVfhVuFyGO4jeXDMU/F+/ALy48MfwgYWJ59/ZTUd8YQEmnTN1+g2SkelHXGAjb/8zlw/Dgf9Yq2no9kqH1pJfFha/0JsGg6MhI8+I/t42DJDW/fnLqYMzZYFzLQSQga0+OomWmwR/ZirLxIsztYnAPDc92sac+ZFm9CF/ra+1sF40+VP0FSfV6XHNzmCErJnH9dm4FnLqFIisrgcRHXVPr/53GpgBgvMB31Il7tgshsaZnpw648dReYdVGI0WcI8Rsl2vGlmDe/RNRnJ+pcZTyGLXwUPcbLF42aVBrOru4hkxvjVLmXo+kuejbjJtG48ZxPXHdKdZMsHpToFhQbVSvfIzs2WxRKsxJ1xZcOjTHfBgHnNY3JtZEY5Uwt4vBx1ALhOCntzONBXQsF1rZLloTpJ7VIdXriWqGy7ZDlXjigw3NWTXCdiOxlkDzOwBlMHCMdFqbhAvLEeISoltCK9tFEws3J/HJd979Z2Dt3jLkZabhnZV7AQB3n90f/QuzMHlwEWo0zKleSdJ8ej5zYAHOHFhgbZzQfwI3qpZq+TpA3+0iWj7qbSxrnkhsFRauUweEqrGS7WKG2EfApuXjvy2/K61jUr0SahssFo1zEPHwvT8vRl1jAFsPVeIPV5zU2lcbmsQVbpcEVR+JkIRD8UGIS5il2poerzPLiIkj3fIy0C0vAyt2HQtty/R58bPzBgIA5n0bHojo9UiWbzZG7fT2GVVzFbNqzGhOtdVwu4iWj6a253apqGvE1X9vXalXhnFZ8j9/rkzpte92kTWLhAHapfJFcbJk+1Hc/84axf7axtZrnur1oEqOXrxA0JW4dm+57lo+ahLJtQGohV8cB2JAJAUW3YJuFxJz3FwaPl7M/NFY9CvIwts/bk0hF286VifdnIzWdNaMNO1jtCYfMf1W3Fun4ZZojvmI3jU3cl/ZEWFV9U2a1VS9XlF8tD3LR2m5clFNrUJiIst2HlO8t/vVydB3u2idV2y75WBl2P46QXykeGOT7aKusmt0ymjO707EruLaJ5AwSqChAKD4IDHkrVtPweBuOXjrx22/5supfTvjs/vOwBghY0Z8qjSrcBokPdWL+T+diIUPTNSdqLXqfIghHKKw0Eqo0Yv5sIuu28Xgs1q9DkFemL89bFtKO4j5EAkE7Jnj7X5zsgzMXnsg9N4fkEMCQiug1yyrpFaobxKQEZNMCUmSVAGnsVcfX+88hgGPzsEf5+rXVNEiEsuHWuxESy+4VUo+Eig+SMwY26cT/nfXaTi5xNpaH22NBuGp3CytVqR35w7oaZBxUpQTHpYuCgFRE5w9qBBnDuiiCEKNp9vFrvtJqzZIYU56SGy1NbeLPyCHiQcZ1lf6BZyV1//dJ5tDr19etAODHp+DyrpGzbRUcWLXGpfodnl7+R7M+HKXpTFEMr15JPX6MpFNluv3lWPB5kO2jnnigw0AgD/Ns1fZVi/exoylO45iwKNz8ML8beaNHaAsmR+VU9iC4oMQlxBdBnrVQJ3w0JRBYdvECUl8ner1YMbNY/DA5NZS6HoBp25hJLTsWj60GNe3U2j8ZovcJRI7DlfhpCc/xXOqySugToU1wb7bRTt7ZcWu45oxNWZlzEXxsW5fub3BOKTZ8tH63iib1sqV/N6fF+OmGcux+2gNAOCjdQfw9MebDC1QTv/LWI1VUfPz99YBUArHaJEI7iAGnBLiEuJiam5RkO1Dfoe0sO1m1csz01r/a3s9UqgQWDRIS9HvOxLxMWlQAU7u2REF2ekt4kM2dLtIUmL5tZ/+eBMq65vwP8EFAjSP0V66qhQ6zgp67fSyicQJWCsQVoz5sEpdo9/2ce+v3hd63fxtu1OsS5xoD5TXoqRTJm5//RsAwMklebrHOf0vo6zz4ayPaKCwJMVvGCEoPghxiSHdc3H32f1RYlLPww0Ulg8NK0umELxqx+1ihF4XRuXZUxys8QIA/Quy8I8bR7eeu+XkdQZul1SPJ6EsI3ol0GVZNsx2UXOkqh7bDoUHguqh1/PR6gZU1IXHa5jFfNgVEQ1NAYx8aq7ttXDufnN16LVaSBoGnJpcyxphHKIoB4DDVfW6x9nNCAkE5LACcrZEk4uK4J+Ld2LD/nL8/vLhofuD1bopsYJuF0Jc5N5zTsBlI3u41p/eLcKjk+0SRLzJeixku+S2ZN2M79tZt43eAndG4sNpoKt6fZtgPw0G2S5uBNXGAhn26z9M+uMiy20/XndAc/uD/1mLN77eHbZdmZ0RfpzRNddi97Eay8Ljd59s0twuSZLlrBGzKymusKyOQTISGEb/ZTaXViq+w4amACb9cSFumrFcUUAuXrEVT83eiHe/2YeFW1qXI1D85uKvPSg+CGmL6GW7BBEtHykeydRNM/vOCXh4ykA8ckF4fIkZRkGlTmNN1Cv7hmI+DCbClDaSwh2QZcMKp5FyvMbeir/iPKTlDrKb3lxe22C57Qvzt+NQRV3Y9vBUW+cVTqvrW4WQOh7GSK8a/ZomP7dIEcuzek8ZdhypxsIthx0HnEZDD9z86nLc9caq5v4TS3tQfBDSFpF0sl2CdBAsHwEZipiP3142NKx9cX4mfnxGX2Q7iFvpo1HSPYhzy4fyuODwjSZCq6vzxpvy2saEWo/EzE1g15V1pMq6+ACUAa1B7LhdzKgWUoPVac6GmSwmwlk8VmzqZp0PN7wjH7RkkLkVQ+MWjPkgJIHppBFsCiif2LTukWLBsvpGv0KsXDm6xNFYtM7zl2tG6LpjAKBQI03YCqkqU03QElJtUGNCbS1JVBZsPowFmxNndV5FPQ2NWd6u2+VQpX4chRZa6b+SyvahniyVq+4aT6RiXZKALCvE1P7ycKuLFlrXRVxWQPzlJWqFU7OspljTNh4VCEkyXr15NE4uycNfrhmhuV8Z8xE+6YqukPqmQNTiIczcKnmZaXj39lPxv7smGLY7tW8nxXv1eLPSm5+TtAImg7QV8ZFoBEwsDHbFx1GDIE4tNpeGB9M2B262vldPlnYm9WoD8WGE+GvSshR06tAqrMX/BqJVy5bbJcqKwHEgbJSg+CAkAZk4oADv3j4e/QqyNfeLNzuzObdbXrppGytodWGl25NLOmJwt1zd/Z2z0kJr0wRRu12spDF7DWI+fjl1sOnxyYqiyJgLbhetdF4jps38BrNW7VNsk2CchWMWJBukvKYR5bWtMTAB2bqYEv+PaV2Xjh1ShbatjUVLjtuWj/X7ylFRZy+mBzBe7yde0O1CSBtEtHbouT1mTRuPgxV16FeQbegaiSdpXg+W/XwSjlUr4wTUVozsdPNblVFaryhe0lM9uHl8b7y0ILyUe7xJ8UiG675EA+US8JG7XeykEQf5+xc7FO8lSTKMUdBbu0aktLwOp0yfpxxbQLYuPoTXWh+pY2aaZltxmQU71gyzll9sPYzr//k1CrJ9+PqRSZb7bR5HYhQWE6H4IKQNYsXycVJxnmkbxyd1CY+n2cWirpKqTrW1Eghr5HZRrDjs9ShWyk0kvPEQHyYxH1or4Vrtzyrq784s4FRxCp3TfaaxwrMs2xAfwo9G6zMplzgQLR/23S4LtxzGdy3VV/WYs74UgP2YmuA4AgaWpHhAtwshbRwrxZCiFfMR6S0smIWTqqqSql752IrbRZKAS0d0t3RetbhJFOKRsfO9Py/GvrJaANpuAq2AUCPs1jABwr8PdbEu9VO707kzIDsr0a+VgixuUVg+mkQxZ9738l3HcOMrX5u2EwX0bhOhokZd0j8BtAfFByFtHcnC/+Jore0S6U0sWH1RPemqx2vF7SJBwk8m9tXeJ/Qna5wvUTCrVeI0e8iMX37YvIiaG0/ETgw3K787rngvQTK0fCjdLs2vq+ubcN0/luHfS3bpnscfMC7RrxyD/vkBpSASf64NNi0fX+88Zmk8Yldn/WGBQbvwcwZUMR8//NcKPP7+ekvnjRaJ+T+QEGIZK7LCjZiPaMiXoEUm3OyutnxYEB+S/sSn6E0Ot6wkCmbl6C+2aNmxS7AQlxOrhRp769ZoI0nA/hZrDGAt4PS1pd9h8bYjeOz9ZiGlNQpZltHgt1Z9VVG7Q8vyIWzy6LhdrGg5u24tAIauOa1d/oCsuGbbDlXhy21HbJ/XTSg+CGmDKGM+zCdSV7JdojBfB90uarGhPlUHn7XwNL2aD+qx27F8DO+hn6njNmaiyGgF4UgIXjc3wk3cCGz87mgNbv33ytD74LiCwkarUnhYATqNcdQ0+C1XbBXdmVoWDD2rht2YD6viQ/0bDsau7DpSjS+2ttaN0RJ/I56ai6c/Vpayd7rukltQfBCSBETL7RIpWoviAeE3WrFcvBF6T93iRCJD6d7onKXvynjp2pN10521iNQtYuZ2SfF4oiICg3Okk0wVNW70oUaWZTw6ax3G/uYzHK2qVxYZa3mtXkFZaxQ//NcKfLB6v+55Gv0BrN9X3vw7Eq7zYxouCr0YCjFG5rdzNpnWPdGKqSktr1MUR9Pi0VnrAAATf78A1//za6zeUxY2riANTQEsVlk64r0WEsUHIW0Qu26URBUfXp1xqbdmpFmzfHTJ1p78xdPIsqyooPrfn4zTH59HsvUkP7pXvuW2Wpg9jaZ47a61ao3gR4zEahEIyCiraYhKVc+ALOO1pbtxpKoBM77cpRk0qhYfery5fI/uvl98sAHf+/Ni/P7TzYrr/NG6Uo0xKccXRMymOV7TiHvfXmM4Hq3sm1Omz8PJT81VbFN/82+v2Kt4v25vGQDrcVjxXguJ4oOQNo6Ve407Rcbcv1npPX2pxZUVy4ckSSjITsdrt4zFeYOLlPtUbcUbr5EwS/FKtjJ6InWLmD2NpnikqNRsCbpd7BYIE7n99W9w0i/nYvXuMpdG1YqYNfKX+dtw5u8WhN4HR5ye6hXaO/scM5c1r/r70oLt5hYmuXnp+u//bYnCSqF2oyzbcdSwGz23i936Ki/M345Xv9xp2fJEywchxDbimi/pFp744n2j0UNXfKjeZ1gRHy3/TujfGSf3zFPuEy0fALKEGBI910/z+Dy2rAGRPk2alYhP8XrcqdmiIjhXR1JjZM6GZuvAxgMVbgxJgdqVUN0grFTbUkBLXFKgttEfcSaWmdgOyDKemr0RX+88hlcW7wxtV8eUmMV9OAk41aK0og5PfLjRsmiJ93IELDJGSBskPdWLLx48E16PZKlmRcK6XfRugOqYj1RrMR/6CDEfMnDWwAKcNbAAQ7rn6rp+ACDVI+m6EVK9Upi/PtIUXrPjU71S2KJrrtDSXaWD0t2xwGwCD8jKa1fTYC2jxQiz/zLiiI7XtFborVOt0msm6KzWUXn3m73mjWBdzMT7gYSWD0LaKMX5meiWl2GprRvaQ6sPsxVFRf50dfgieXoptOqnzkwLMR/KZc319wHNFoRXbhqN+845wdCS4PXou10yNARRpOIjbgGnkPH+6n347NtD7nfuAmYGmeYKnq2Nahv8EWfdmFkQxPPVCbVD6lR1RMyGYbXoWbVFQWXV8kHxQQiJOj07ZUbcR6S3qouGd8OVo4pD7ztn+fCH7w/XPpfa8uGzEvPR+lr9pKxYp0MlJ4zcLile/YBTLUEUqSk71STgVJKik/Isy+GFvhIJM8uHuo5FTaNxpogVVphcD3FI9U2twqCuyZ7VpdFmbIcZVsWMN86ptnS7EJIETDuzH8pqGjFlSNe4jkOc+D//6Rm6ZdPV86ulgFOVa0WxT9LfZ+SS8no8upaP7h0zUFpRp9gWadl2M8uHPyBHHMug5S4K2FjzJB6YfWZZVgal1jT43XZMaZxT2/JhtYJqECM3iZH1ZkRJnub2thLzQcsHIUlAZloKfn3JUEzo39lxH248cYvBeEZxHGGWj1R7z0nqbAejoRvFfKR4pLDwiu+P6oGxvfPx5EWDw9qnRRhwamYKD8h2HF3aaKXzynC25kmsMLV8yLIiy6PZ7RLdMYnnEy0f9RqWj+2Hq3DZS19hweZwt5ZRzIfRZ9CzktHtQgghKsSnQiMrgTrmo4MFt4uIOt1QWedDdS6Du6DXI+HK0cWKbVNP6o63fjwOJRqurEgtH2YxI01+OeJYUy3riiwbT1qn9ImsfkmkmImPPcdqFIJTSwC4jZiWbGb5mPb6N1j53XHcNGN52D6jz2aUNqsnFq0GnNLyQQhJGqz6w7UCRGffOQGzpo23dIxRwKm6hLmR28UfkHH6CV1w76QTQtuCT4xaRa2inWrrxropWp9XhrH4iMZTcmGOD9edUmKprdnqsN/782LFRN3QFIi620UZcGoc87GptNLROYwyZZoCAc16JrR8EELaFVp1D+yato384W//uLXSqJYeGNI9FycV5+kerwgqDQs4bd3rU7l7jNwuweJRA7u2llhPCYkPL358Rh/06dwhtK+DxUqsephaPgKyrQwjLTSzlmRZ90n6hMIs3HRq74jOqcXDUwahT+csS23NMlf8AVll+Yi+C0kUBuJrdaptJBgVS2vyy5qWkXpaPggh7QpXYj70b8xjeoum/chOFmbKFrpTWyz0tEdxfkZI7IgBr+IT48NTBuGpi4eE3udlagfQWsXsadQfCLhQPAtY+8S56C2IJj23S58uHfDRXadh0qACfHz3afjXD8ZEdnJxHJJ1S5EVg4/YZsWu43hq9kaHI7OGnjBQp9pGgpHlo8Ef0LSEWbd8cGE5QkgbxWqdkSCWVxR1oj2Eg8LcLsJrtfjQmvCzfSlY8NMzQyW7xZoe6oBN0Y3RMTMNkWA2GfcvzDa0e1gtQ5+TnoouwoJ6MrSzXVI9HqR4PZAkCYO65tiOvTHCI0mWi99ZWR1WnIj/vfQ7x+Oyip4wsGL5kGUZ2w9XodHvXEw2+WXN6/LRugOWjo+35YOptoQQ2/z7ljHYdaQaI3t2tHWc9eXM7SMeE1bnQ5jkfCnKCVRzApSUokRcN0QtVkQ3SKTiQy+D4ZdTByPF48G5JxbquiAkCfjhhN740+fbDM8hCe2DyLJ2AKP60rhZKdcjSZYnQCuxLlYEipvoWz7MxcfstQdw5xurcM6JhY5XAW70BzQF0PsGq/aKeLmwHCGkLSDeqk7r3wXXj+tluw+r/vBI5zijVFtfarjl4/yhRRhYlK3ZHlBaFNTWier61s8UsdtFZ0I4uaQjrhlbYrionEeScOfZ/U3PEexCFBJm2S6h8bn4tOyRjAu8iVjJ4HAjGNcOupYPC9fxpQXbAQBzNx50vAheo192fCwQf8sHxQchJGZY9YdHuoKuUbaLVpbKi9eOxFu3tga8qo/P0In5AJTroVgprz60e67uvlSdCUFcNE1vupFazn9CoVkQZ/M5RCNLQCfgVC123LR8SDYsH1YWvHNqQXCKnqXFioirFUS408X8mgLaMR9WiXe2C90uhJCY0T0vHUeq6k3b2Y0lAczKq+u7XYKIFo0mVW6nGPOhnnMKc9JDr63cz9+7/VTM33wYjf4Abn/9G8U+PUuAKGr05tigMGiyuFCZWkhoTZrq0bg5YXk9kuX+rHymSKwATohkBeDq+tby707dRY1NgYgEV7wtHxQfhBBLGJn8rfL8VSPw1OyNuP3Mvpr7/3njKHz27SHcPL6X7b6VqbaqfSaWD0ApPtRPlKLlQ+0COLVvJzxx4YkY1DXHNFilc5YPKV4PzjmxUHO/3lySpjNmkaAlo9GkKEbwWojfp9U6FO7GfFgXM2afCbCWEeMmkVgdaoVF4pz20xiQTeufGGHV5RUtKD4IIZZw41bVq3MH/POm0br7zx5UiLMHaU/MZkiKbBfrMR9BxGBP9VNtmmB5UAsBSZJw0/jmOhiHK42tOmZzt95ElGbBnRO07phZCYJD0Jt7PFLrRK4eb4QFXFXnkQxrrIhYecCPdcxHJOeraXRBfPgDYRY6O8Tb8sGYD0JIu+MGdTCswvKh7XYRnwS1FqZ74sITMe3MvujbRT+mwux+fvUY44qeemZ0K+IjeG6z4EytgFMRMbMnmtkukoblY7hBETkzYlFYTCQS8SEe67QfWTZeF8YM1vkghBAXEKexfgVZ+PCOCYr9OenNhl49l4cZN43vjQcmDzQeg8nkfOdZ/Qz3F2T7NLdbcbsEz20WixC0kOgJJTG+RR3466ZtwSOFx3zcMsF5JdWahibzRi7iRDSkeCR8sEaZChuJiIlkDZt4Wz7odiGEWMLFh96ooB5fVnrr7U2ChHn3T8Sm0gpM6Od8ZV8zjO7nXo+kmw0jSc3xMHo1PKyJj+Z/Td0uGjEfIukGqw27iZb4sOqG0aKqPrbiw0nAaVNAxl1vrFJsiyRoNJJqqvHOdqHlgxDSZnn2yuG6+8R7qyQBXbJ9OK1/F1cCZ/UwShE2esId37czLhreTdetYWWiuP+c5sXvTN0uLf/qdZkuxMSs21eu2OdmNqtWwGkkMSU19dFfyTYaRGL5sFpKXYt4Wz4oPgghlkg0y8clI7rjkhE9dPeLE3mshi45vKMaxWEs+OlE0+MX/HQibjy1FwALbhcp6HbRvip6MTFWMFr4T02K16Mh1px/U0erzVO4E5F4iQ9aPgghxAHqW2dOurK6qKSwfMTmRuvUMhCcCLSe/HsJC8Dp0atzh9BnNJvMzAJOayNYlbVTB+vl5TNSvSivbVRsi2Q+XL7ruPOD40hE4sPfdmM+KD4IIZaItOqo2wRv2c9fdRKGF+fhl8LqsoDK8hGjoafrpPGaEYx1iIVICp1C51RHDYrAmS1cZ3WVWgDISPOEBYnazaZJT/WgjwVxlqikeqWI1qSJyPLhZt60Ayg+CCFtmqkndcf708aju6oqajzcLr4ULxY9cCZ+fHqf0Lbg4nuTB+tn2Zi5QqKB3rmuHF2se0y3vAw8MHkAHpg8QHO/lfLyQdJTvTh/aFflmGzOSF5Jgi8KAbL9C8xK1LtDiscTUaXUyjrnQbaRBPe6gevi44knnoAkSYq/gQON09MIIcQuZrdOdcBprCjplIkeHVuF0HNXnoTnrzoJv7/CPDg2Fg+jQQuW3iX52XnG9+tpZ/bTrUBrV3x08KVgSPecsLFZxeORHFubjDi5pKPrfWqR4pEicrs88J+1jo/NSGuHlo/BgwfjwIEDob/FixdH4zSEkBiSaAGnZihdGLEdvDid5KSnYupJ3ZGdrr/ibTDmI5ZuF71Tpdiophp2rI04gmA9EbvusX/cMCr02uuRFHVJmvuzPARdrKQ2u4FflmNemTVIli+yFZgjJSp1PlJSUlBUVBSNrgkhxBLxjKcT3fhWXAkeF90u3fMysK+sVne/G5clEuESJFhPRBRcVj7/JKFInFeSwuqSSJIUcU5wrMRHU0COiSzOTPOipkEZnJrli2+Zr6hc4a1bt6Jbt27o06cPrr32WuzevVu3bX19PSoqKhR/hBASKcqJLLZPl2KxMCsTarC0u1ow3XaG9gJ8Rvz7ljG4anSxblxGcLKPZNLT+0ypNgJOvRqfWatfo8un5XZxw5IQM8tHQA4LOJ35w7Gun6eDhtDITm9n4mPs2LF49dVXMWfOHLz00kvYuXMnTjvtNFRWaq+aOH36dOTm5ob+iov1g50IIcQq4kRm9UHYLWuJeDor9RRCMR/CmH9zyVA8NMV+vFyfLll4+rJh6NkpU3N/8Ax2rBRq9D5SioP1QszcLv26ZIUFE7eeT0J6BHVJ9LATuxIJ/oActj7Lqf06Y2zv/NB7r0fC1WMimxe1vq52Z/mYMmUKrrjiCgwbNgyTJ0/GRx99hLKyMrz99tua7R9++GGUl5eH/vbs2eP2kAghLhCrWhluIRb8svosHMmErIeR5ePErs3Blped3FwsTVxxt2NmZD553eDNls2R1HnQ+y3YsXwEMQsMDsgyTu6pHQDqkSSkm6T/OhlParzXPRGuY4pHwvRLh7l+jqw4Wz6ifva8vDyccMIJ2LZtm+Z+n88Hn097MSVCCHGKE8tHmtcTUe0ErfMZWT7evf1U7CurDa2Um9+h9V5oVexdMqK75na9w1stH84nWF3Lh4M+zWI+ZFlfKHk9kqUVf7XonJWGI1UNmn164l2AS7gOblhhtK5ru7N8qKmqqsL27dvRtWtX88aEkIQl3k+DdhGHq7dgmxq3fP3i2YwuW3qqNyQ8AKBTVmuF0AaTNVoA4MuHzsIfdFJ4xfO+d/upodfByd6Ji0Tdh8iQ7jkO3S5Cvxr7ZegLuBSP5HgSvWp0Ca4ZW6IxHilu1T+D8Svi+SMRiUG0hKgvRnEterh+9p/+9KdYuHAhdu3aha+++gqXXHIJvF4vrr76ardPRQiJIWcPKsRJxXm6NR5ijsk9WWH5sNilE7eBFqLYseOuyhYm0rKa8KdyNd3zMnSf0sW4y5L88PgPtz7rGSd0wU2n9sLfrh9luc/T+reuLCx+T1qfJSDLumLA45Ec/x4Dsoz8zPBy8LIcn3VPnr50KObeewYA5fkjEYlGxNuN6rrdZe/evbj66qtx9OhRdOnSBRMmTMDSpUvRpUsXt09FCIkhaSkezJo2Pt7DaMVEUUgKy4e1LmMVaKiHOCEcqzYXH0ZUCdUvxRojwTN4XZrUenbKxBMXDQZg/fr9381jQq8V4kMn5kNPDHglCfk21pNR9qstwBoDgbiIj0tO7h5a1E88vxsiMRFtlq6LjzfffNPtLgkhxDZKy4dFt4tL4sONpedFd4wTKupaF20T3UnBy+KWa8FJQrNo4TBbANDvl3HnWf0xZ30pvq8q/e7xSI6f4GUdi4o6xuT0E7pg0ZbDjs5hBzHTyeuy2yUR4douhJC2iQ23i9VZ0b2YD+fq45N7Tsf0S4figqGRxcnprfsRzIIxmtTsPPmLk//hSvvL2putwVPfFEBRbjqWPzIpVPp94oBmS7qWy+Uqg7VpRAKyrOuyEq1CV46KTfkH8ZqL5091wUIVbxeLFhQfhJC2icn87rGvPVxzu0Ri+RhQlI2rx5REnHGhKz4MLB8f3XUaAOeLjt0wrif6dumgmLBzM4xThs2KjNU1NlfmFK/Hy9ePwpx7TsMVI3so2nbMTMUZJ7S6+I3W0zHOotEeX7TwSEqBIOrC9mr5iG+uDSGERAknT3uxqmwZCy49uTte+XJnaFVdNVo1TU7s1lx3xOMB4A/bbUrPTh0w7/6JkGUZMmT0L8jG3xbtMDzGY5JqW6+R+pyW4sHAopyw7Z2yfAqR0jlLPx4kYBBYKloeYmE0UI9DPH+0Ak7jDcUHIaRtYmNSsB5w6lK2iyu9RMaQ7rn4+udnhwVktqba6n/WSNeYkSQJz1zebHU4XFWPlw0EiCgStU5rZ8l5f0AZx2H0OYyyaFIUMSn2rkWKR8LYPvn4cttRy8eoxynqQq3f5IDCbGw+qF01XIsE9LrQ7UIIaaPYmOEtB5y6VKrbjYBTM6y4Awpy0sMsHMHDjFxMdsSHWdP7zz0BN53ay+Bc1vsyoykQUFg+PJKka/2QZRlenWug7sMqRTnpWPHoJPzucn13jxZqi5si5kM1xvMGF6F7x9Zy83kWKuFKUuIJEIoPQki7x3qFU7csH9FXH07TQYOTkFfhnvDh0QsGhd7b6Vq3jHsLvhQvLhyuHzzrdTjRaxEIKONVPB5g4QNn4o/fDxcDAYOYD4Xlw8b5/bKMvMw0dMvLwOWqeJQgaV4P3p82XrFejbrgl6g31DEfaSkexTUrykk3HZcECTN/eIqVjxAzKD4IIW0TO24Xi+0SIeDUKpHWohDN+csfORs/PK2Po76t6AXRdaEWAuLxkYqPJlWNDo8koYMvBf0KwtOWDeuHeJQCxioBwUWkF+zawefF8OI8hajwqSxuooBS/yZlKMVRN51F99SM69vJUrtYQfFBCGmT2KnJYbW8eryLjNnBaUZKa7aLGFSpjjlw10Yv9jZxQIFqPMYxH3bwB2RNS4rW9zq2Tyddy4eiLooNleu38DsL6hPx3EZulzxVFVZZJZq65lqwfCSYywWg+CCEtDEevWAQ+nTpgPvOOcHyMZbdLm7V+YiB6cNuKm7QPH/OoCIAxp/VTpCllZbKxeOU+xRukghnyZL8TJX4aP5XtPLMvnMC/nrdSFw4rCsGFGVr9tMhrTUXw86Q/H7t731M7/zQ66B1RBR/Rm6XfFVMhzpFuCDbXHwkIhQfhJA2xQ9P64PP75+IAgu+7iBWYzBOdck0HQu3i90KpR/cOR7PX3USfjKxLwDgnBMLMbAoG1ePCS+iZceqYsntomivPED8GEapsUa8d/upmDKkCM9fNUJzrRhxou/RMQPnDSmCJEkY3C0X/7xxFD6++zRFf5lprW4QO4JIbfnokt28SvGkQa3WnkBLG9HtYmT5CK50PLxHLgDg8pE9FPtzMsyTVhPQ8MFUW0JI+2VgUTa2HKzE6F755o3RvDy91yPhpOK8iM4bi1Rbu66Rgux0TD2pe+h9eqoXc+45XbNtNAtrqefyaWf2w6zV+3HdKSWmBcn0GFHSES9dNxIAcFRYEycoHMRrpRY/Zw8qDOuvg8+h5UOVFvy/uyZgxa7jOPfEQvzmo03NbYLiQ3S7eA0sHx2ar8nbt43D3uO16NslC3PWl4b2W1nVNxErnFJ8EELaLf+76zQ0+gNIT7WWQitJkmKCdsq4vp2AuRF3Y0ikLgrDvt2O+TAIKu1fmI1NT52n+R1dOqI7brK5am2KhtvFbpVQ0fJhJ+ZDbfEqyE7H+aoy+YGWmmmiIPKlmls+fCne0Ho/XuHziIsG6qF26yQCFB+EkHaL1yPB63GndocdRvfKx39uG6e5lL1bRHPlVadru+i2ESZwrdai8MhI9aK2paT6H688yfI4gmhVTM2wKD6DiOKjMRDAO7eNw39W7MVbK/YYHmct4DTodmkVBGGWD+EzaLlVOgjjy043n8afdXAdow3FByGERIFRFl09Tomq5cP1gFPrfQ8oysbqPWWWz69G6WJp/jcvMw13nd0fkGVLrp1MIeC0rsGPMwcUYHSvfHPxYaEaa0DD7aJOtVXEg2hk6mT5Wj+DmfiYd/8ZEa+QHA0oPgghpA0SzQXHbBlVbA7DTNecPbAgQvEhvm49mZ3sKPG4oBXGLUKptqLlQ+UWEQWaVlaSKDg6mMR8OE3JjjYUH4QQ0gaJ5qQSTauKWdc/PqMvSivqcFr/zo76F+Ml3PgcVsTHBUO74n/rDuDGcT0t96u0fHh092mJjyxBfNh1KSUKFB+EENIGcTsoVMT1ImM23C5pKR78+pKhjs+lrBti/ThJ0k6R1hJ5vTplIiADu4/V4Kmpg3HFqGJcPaZEUc/DdJwGAsNjkAkDKOuQmImPRFjkUAuKD0IIaYMkiuXDSjaIWcCpm4jl0O2kmHokSREw+ugFg7Bwy2FcPCI8+ykzLQUfqWqDTLBpqUk1LK/e+lrL8iEKl4w0Y/FhpQJqPEi8/BtCCCGmDOqqXZ3TDW5uSW+14vqwtrZL6+tounQA5wvVqcXcD0/rg3/fMlYzBbgwx+d8gMHzefRjPrwG8SCAWrhoT+MXDe+Gbx47x3Kaeayh5YMQQtoQs++cgP+s3It7JvWP2jkuH9kDQ3vkok9n97Mkoh3/KIoPtyu1BrG6mJsRqQYxH6K7SGtdmo4dWivBitadNK8HDf7mQiJ5manI7+CsYmwsoPgghJA2xJDuuRjSPTeq55AkCQOLcqy1tdAmX2eyjAZO3VF2rCRuiA9RJKnTZcWMXS3Lx4jiPPzotN5hdWS6ZPuwr6wWQGKWVBeh+CCEEOIYreXq1RTmpOMv14wwTQt1A3FSt7qmj/o4PYb3yMWaveWYelI3R2MTSfEaiA9BfWgFnEqShEcuODFse1OwfCqMRZ7ddYGiAWM+CCGE2Oa/PzkVPz9/IC62WI7+e8O64cwBBeYNI8RpFpBZ4CbQvL7KqsfOQY+O9ivX/uWaEUj1Snjx2pMBKBe7U5dI95uIDz3E47QsOU9dPCQ0lnhDywchhBDbjOzZESN7doz3MMIQn+rtrC789xtGYdrr3+CRCwbptvGleMMyU6zyvWHdMHlwUSiGw9jtIogIG2Kq0S+Kj/D915/SE98f1cPxZ3ATig9CCCHtBvGJP2BDfZxUnIcvHzorGkMKIQaPpijEh9Ly0WShTLsWCsuHjmhJBOEB0O1CCCGkHSGKj0QtsAUoy6urLR9W1ojRotEvxHw4G1bMoPgghBDSbkj1SqGqn91dyEqJFmIoh1HAqR0UoiXB1QfdLoQQQtoNkiRh1ePnwB+QE7bAFgBs2F8Rep2XoazH4bcTrCLQFJDRqUMajlY34NwTCyMaX7Sh+CCEENKuSGTREeTCYd2wYPNhnDmgS1gtD6eWDwCY/8BE7C+rtVynJV5QfBBCCCExZupJ3dCrcyaG98gL29fXQu0UPXLSU5FTlGreMM5QfBBCCCExJsXrwcie2qvgXjisGw5W1GFUL+ur5LY1KD4IIYSQBMLjkXDr6X3jPYyowmwXQgghhMQUig9CCCGExBSKD0IIIaQNc1r/zgCAMb3bTowIYz4IIYSQNsyfrx6BD9bsx/eGRb7abqyg+CCEEELaMHmZabhhXK94D8MWdLsQQgghJKZQfBBCCCEkplB8EEIIISSmUHwQQgghJKZQfBBCCCEkplB8EEIIISSmUHwQQgghJKZQfBBCCCEkplB8EEIIISSmUHwQQgghJKZQfBBCCCEkplB8EEIIISSmUHwQQgghJKYk3Kq2siwDACoqKuI8EkIIIYRYJThvB+dxIxJOfFRWVgIAiouL4zwSQgghhNilsrISubm5hm0k2YpEiSGBQAD79+9HdnY2JElyte+KigoUFxdjz549yMnJcbVv0gqvc2zgdY4dvNaxgdc5NkTrOsuyjMrKSnTr1g0ej3FUR8JZPjweD3r06BHVc+Tk5PCHHQN4nWMDr3Ps4LWODbzOsSEa19nM4hGEAaeEEEIIiSkUH4QQQgiJKUklPnw+H37xi1/A5/PFeyjtGl7n2MDrHDt4rWMDr3NsSITrnHABp4QQQghp3ySV5YMQQggh8YfigxBCCCExheKDEEIIITGF4oMQQgghMSVpxMcLL7yAXr16IT09HWPHjsXXX38d7yG1KaZPn47Ro0cjOzsbBQUFuPjii7F582ZFm7q6OkybNg2dOnVCVlYWLrvsMhw8eFDRZvfu3bjggguQmZmJgoICPPDAA2hqaorlR2lTPP3005AkCffcc09oG6+ze+zbtw/XXXcdOnXqhIyMDAwdOhQrVqwI7ZdlGY8//ji6du2KjIwMTJo0CVu3blX0cezYMVx77bXIyclBXl4ebrnlFlRVVcX6oyQsfr8fjz32GHr37o2MjAz07dsXTz31lGL9D15n+yxatAgXXnghunXrBkmSMGvWLMV+t67p2rVrcdpppyE9PR3FxcV45pln3PkAchLw5ptvymlpafIrr7wib9iwQf7Rj34k5+XlyQcPHoz30NoMkydPlmfMmCGvX79eXr16tXz++efLJSUlclVVVajNbbfdJhcXF8vz5s2TV6xYIZ9yyinyqaeeGtrf1NQkDxkyRJ40aZK8atUq+aOPPpI7d+4sP/zww/H4SAnP119/Lffq1UseNmyYfPfdd4e28zq7w7Fjx+SePXvKN910k7xs2TJ5x44d8ieffCJv27Yt1Obpp5+Wc3Nz5VmzZslr1qyRL7roIrl3795ybW1tqM15550nDx8+XF66dKn8xRdfyP369ZOvvvrqeHykhOTXv/613KlTJ3n27Nnyzp075XfeeUfOysqSn3/++VAbXmf7fPTRR/Ijjzwiv/vuuzIA+b333lPsd+OalpeXy4WFhfK1114rr1+/Xn7jjTfkjIwM+W9/+1vE408K8TFmzBh52rRpofd+v1/u1q2bPH369DiOqm1z6NAhGYC8cOFCWZZluaysTE5NTZXfeeedUJtvv/1WBiAvWbJEluXm/ywej0cuLS0NtXnppZfknJwcub6+PrYfIMGprKyU+/fvL8+dO1c+44wzQuKD19k9fvazn8kTJkzQ3R8IBOSioiL5d7/7XWhbWVmZ7PP55DfeeEOWZVneuHGjDEBevnx5qM3HH38sS5Ik79u3L3qDb0NccMEF8g9+8APFtksvvVS+9tprZVnmdXYDtfhw65q++OKLcseOHRX3jZ/97GfygAEDIh5zu3e7NDQ0YOXKlZg0aVJom8fjwaRJk7BkyZI4jqxtU15eDgDIz88HAKxcuRKNjY2K6zxw4ECUlJSErvOSJUswdOhQFBYWhtpMnjwZFRUV2LBhQwxHn/hMmzYNF1xwgeJ6ArzObvLBBx9g1KhRuOKKK1BQUIARI0bg73//e2j/zp07UVpaqrjWubm5GDt2rOJa5+XlYdSoUaE2kyZNgsfjwbJly2L3YRKYU089FfPmzcOWLVsAAGvWrMHixYsxZcoUALzO0cCta7pkyRKcfvrpSEtLC7WZPHkyNm/ejOPHj0c0xoRbWM5tjhw5Ar/fr7gRA0BhYSE2bdoUp1G1bQKBAO655x6MHz8eQ4YMAQCUlpYiLS0NeXl5iraFhYUoLS0NtdH6HoL7SDNvvvkmvvnmGyxfvjxsH6+ze+zYsQMvvfQS7rvvPvz85z/H8uXLcddddyEtLQ033nhj6FppXUvxWhcUFCj2p6SkID8/n9e6hYceeggVFRUYOHAgvF4v/H4/fv3rX+Paa68FAF7nKODWNS0tLUXv3r3D+gju69ixo+MxtnvxQdxn2rRpWL9+PRYvXhzvobQ79uzZg7vvvhtz585Fenp6vIfTrgkEAhg1ahR+85vfAABGjBiB9evX469//StuvPHGOI+u/fD222/j9ddfx8yZMzF48GCsXr0a99xzD7p168brnMS0e7dL586d4fV6w7IBDh48iKKiojiNqu1yxx13YPbs2Zg/fz569OgR2l5UVISGhgaUlZUp2ovXuaioSPN7CO4jzW6VQ4cO4eSTT0ZKSgpSUlKwcOFC/OlPf0JKSgoKCwt5nV2ia9euOPHEExXbBg0ahN27dwNovVZG946ioiIcOnRIsb+pqQnHjh3jtW7hgQcewEMPPYSrrroKQ4cOxfXXX497770X06dPB8DrHA3cuqbRvJe0e/GRlpaGkSNHYt68eaFtgUAA8+bNw7hx4+I4sraFLMu444478N577+Hzzz8PM8WNHDkSqampiuu8efNm7N69O3Sdx40bh3Xr1il+8HPnzkVOTk7YJJCsnH322Vi3bh1Wr14d+hs1ahSuvfba0GteZ3cYP358WLr4li1b0LNnTwBA7969UVRUpLjWFRUVWLZsmeJal5WVYeXKlaE2n3/+OQKBAMaOHRuDT5H41NTUwONRTjVerxeBQAAAr3M0cOuajhs3DosWLUJjY2Oozdy5czFgwICIXC4AkifV1ufzya+++qq8ceNG+dZbb5Xz8vIU2QDEmJ/85Cdybm6uvGDBAvnAgQOhv5qamlCb2267TS4pKZE///xzecWKFfK4cePkcePGhfYHU0DPPfdcefXq1fKcOXPkLl26MAXUBDHbRZZ5nd3i66+/llNSUuRf//rX8tatW+XXX39dzszMlF977bVQm6efflrOy8uT33//fXnt2rXy1KlTNdMVR4wYIS9btkxevHix3L9//6ROAVVz4403yt27dw+l2r777rty586d5QcffDDUhtfZPpWVlfKqVavkVatWyQDkP/7xj/KqVavk7777TpZld65pWVmZXFhYKF9//fXy+vXr5TfffFPOzMxkqq0d/vznP8slJSVyWlqaPGbMGHnp0qXxHlKbAoDm34wZM0Jtamtr5dtvv13u2LGjnJmZKV9yySXygQMHFP3s2rVLnjJlipyRkSF37txZvv/+++XGxsYYf5q2hVp88Dq7x4cffigPGTJE9vl88sCBA+WXX35ZsT8QCMiPPfaYXFhYKPt8Pvnss8+WN2/erGhz9OhR+eqrr5azsrLknJwc+eabb5YrKytj+TESmoqKCvnuu++WS0pK5PT0dLlPnz7yI488okjf5HW2z/z58zXvyTfeeKMsy+5d0zVr1sgTJkyQfT6f3L17d/npp592ZfySLAtl5gghhBBCoky7j/kghBBCSGJB8UEIIYSQmELxQQghhJCYQvFBCCGEkJhC8UEIIYSQmELxQQghhJCYQvFBCCGEkJhC8UEIIYSQmELxQQghhJCYQvFBCCGEkJhC8UEIIYSQmELxQQghhJCY8v+xOwAR9ffn0AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist)"
      ],
      "metadata": {
        "id": "DL6xQUENDjVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=0\n",
        "loss=0\n",
        "gcnn.eval()\n",
        "for i in range(3700):\n",
        "  pr1=Data(x=protein_embed_1[i],edge_index=protein_1[i],batch=batch_1[i])\n",
        "  pr2=Data(x=protein_embed_2[i],edge_index=protein_2[i],batch=batch_2[i])\n",
        "  output = gcnn(pr1,pr2).detach()\n",
        "  loss+=np.abs((labels[i])-output.item())**2\n",
        "  pred= (output.item()>0.5)\n",
        "  res+=labels[i].item() == pred\n",
        "\n",
        "print(res/3700)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ea4yiTHEAmv",
        "outputId": "5b7a1f23-af4b-495a-ce0e-cad5c98a5c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.812972972972973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./*.pkl ./drive/MyDrive"
      ],
      "metadata": {
        "id": "LXHBpX_fEFdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gcnn.state_dict(),'gcnn.pth')"
      ],
      "metadata": {
        "id": "xmS79iwiHOHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GCNN()\n",
        "model.load_state_dict(torch.load('gcnn.pth'))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsKjA1t1IUbI",
        "outputId": "7b053cae-b44e-4c36-c3e9-055b260cd09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCNN Loaded\n",
            "GCNN(\n",
            "  (pro1_conv1): GCNConv(7, 7)\n",
            "  (pro1_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (pro2_conv1): GCNConv(7, 7)\n",
            "  (pro2_fc1): Linear(in_features=7, out_features=128, bias=True)\n",
            "  (relu): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            "  (fc1): Linear(in_features=384, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part two"
      ],
      "metadata": {
        "id": "UMg8JwQR-C1O"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}